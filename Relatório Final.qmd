---
title: "Banco de dados: Boston House Prices"
author: "Fernado Bispo, Jeff Caponero"
format:
    pdf:
      toc: true
      toc-title: Sumário
      colorlinks: true
      documentclass: report
      papersize: letter
      number-sections: false
      geometry:
        - top=30mm
        - left=30mm
        - right=20mm
        - bottom=20mm
        - heightrounded
      fig-pos: "H"
      fig-align: center
      lang: pt-BR
      # fontfamily: libertinus
      fontsize: 12pt
      include-in-header:
      - text: |
          \usepackage{caption}
          \usepackage{fontspec}
          \usepackage{xcolor}
          \usepackage{indentfirst}
          \captionsetup[table]{name=Tabela}
---

```{r pacotes&dados}
#| echo: false
#| warning: false


# PACOTES ----

if (!require(pacman))
  install.packages("pacman")
library(pacman)

pacman::p_load(tidyverse,  janitor, stargazer,  sjmisc, summarytools,
               kableExtra, moments, ggpubr, formattable, gridExtra, 
               glue, corrplot, sessioninfo, readxl, writexl, ggthemes,
               patchwork,  plotly, lmtest, olsrr, gglm, ggplot2,
               tidymodels, GGally, skimr, performance, ipred)


dados <- read.csv("boston.csv")

## Arrumação ----
dados <- dados|>
  janitor::clean_names()

dados <- dados|>
  mutate(
    chas = forcats::as_factor(chas),
    rad = forcats::as_factor(rad))
```

# Introdução

A busca pela moradia própria é o desejo da grande maioria das pessoas, contudo a conquista desse bem nos grandes centros não é tarefa fácil.
Levando isso em consideração a procura por imóveis na região metropolitana torna-se uma opção viável economicamente, mesmo havendo penalizações no que diz respeito a distância e congestionamentos.

O objetivo deste relatório é trazer a luz as análises e conclusões acerca da utilização das técnicas de regressão linear a fim de determinar o preço das casas em Boston, baseado nos dados fornecidos pelo conjunto de dados obtido.
Neste primeiro momento, em que se utilizará a regressão linear simples, se buscará determinar uma função que descreva a relação entre o Valor Médio dos imóveis e o Percentual da população de "classe baixa".

Composto por 506 observações e 14 variáveis, o conjunto de dados, publicado no *Jornal of Environmental Economics & Management*, vol.5, 81-102, 1978.t, traz inúmeras características que servirão de parâmetros para resolução do seguinte questionamento: O valor médio dos imóveis é influenciado pelas diversas características externas observadas?

# Metodologia

## Sobre o conjunto de dados

Os dados utilizados apresentam os preços de 506 casas em Boston publicados por 
[*'D. Harrison e D.L.Rubinfeld'*](https://www.researchgate.net/profile/Daniel-Rubinfeld/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air/links/5c38ce85458515a4c71e3a64/Hedonic-housing-prices-and-the-demand-for-clean-air.pdf) [1] e usados por  [*'Belsley, Kuh & Welsch'*](https://www.wiley.com/en-us/Regression+Diagnostics%3A+Identifying+Influential+Data+and+Sources+of+Collinearity-p-9780471691174) para avaliar a demanda por ar limpo no valor de casas do município.

Os dados podem ser acessados na plataforma para aprendizado de ciência de dados [Kaggle](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data) através do link:

<https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data>.

### Variáveis a serem analisadas

A amostra contêm 14 atributos de casas em diferentes locais nos subúrbios de Boston no final dos anos 1970, sendo duas delas classificadas como categóricas e 12 como numéricas.
O objetivo é o valor médio das casas em um local (em milhares de dólares - k\$).
As variáveis presentes no banco de dados são descritas a seguir bem como a forma que estas variáveis serão representadas ao longo deste relatório a fim de facilitar o entendimento:

1.  CRIM: Índice de criminalidade per capita por bairro.
    Taxa de criminalidade por cidade.
    Uma vez que o CRIM mede a ameaça ao bem-estar que as famílias percebem em vários bairros da área metropolitana de Boston (assumindo que as taxas de criminalidade são geralmente proporcionais às percepções de perigo das pessoas), ele deve ter um efeito negativo nos valores das moradias.
    Será representada como **Índice Criminalidade**.

2.  ZN: Proporção da área residencial de uma cidade dividida em lotes com mais de 25.000 pés quadrados.
    Uma vez que tal zoneamento restringe a construção de pequenas casas em lotes, esperamos que o ZS esteja positivamente relacionado aos valores das moradias.
    Um coeficiente positivo também pode surgir porque o zoneamento representa a exclusividade, a classe social e as comodidades externas de uma comunidade.
    Será representada como **Prop. Terreno Zoneado**.

3.  INDUS: Proporção de hectares de negócios não varejistas por bairro.
    O INDUS serve como um *proxy* para as externalidades associadas ao ruído da indústria, tráfego intenso e efeitos visuais desagradáveis e, portanto, deve afetar negativamente os valores das habitações.
    Será representado por **Área Industrial**.

4.  CHAS: Variável fictícia categórica que representa imóveis próximos a margem do rio Charles (1 se o trecho margeia o rio; 0 caso contrário). Esperamos que o CHAS esteja positivamente relacionado aos valores das
moradias, uma vez que podem denotar imóveis de mais alto padrão. 
    Será representada como **Margem**.

5.  NOX: Concentração de óxidos nítricos em pphm (partes por 100 milhões). Como o aumento dos valores de NOX representam uma piora da qualidade do ar, esperamos que esta variável esteja negativamente correlacionada com o valor dos imóveis. 
    Será representada como **Índice Oxido Nítrico**.

6.  RM: Número médio de quartos em unidades proprietárias.
    RM representa espaço e, em certo sentido, quantidade de habitação.
    Deve estar positivamente relacionado com o valor da habitação.
    Verificou-se que a forma RM² fornece um ajuste melhor do que as formas linear ou logarítnica.
    Será representada por **N° de Cômodos**.

7.  AGE: Proporção de unidades próprias construídas antes de 1940.
    A idade da unidade geralmente está relacionada à qualidade da estrutura e portanto negativamente correlacionada ao valor do imóvel.
    Será representada por **Idade**

8.  DIS: Distâncias ponderadas para cinco centros de emprego na região de Boston.
    De acordo com as teorias tradicionais de gradientes de renda da terra urbana, os valores das moradias devem ser maiores perto de locatários de emprego.
    DIS é inserido na forma logarítmica; o sinal esperado é negativo.
    Será representada como **Dist. Empregos**.

9.  RAD: Variável categórica que representa o índice de acessibilidade às rodovias radiais.
    O índice de acesso rodoviário foi calculado com base na cidade.
    Boas variáveis de área de estrada são necessárias para que todas as variáveis de poluição não capturem as vantagens locacionais de estradas.
    O RAD captura outros tipos de vantagens locacionais além da proximidade do local de trabalho.
    é inserido na forma logarítmica; o sinal esperado é positivo.
    Será representada como **Acessibilidade Rodovias**.

10. TAX: Valor total do imposto sobre a propriedade (\$/\$10,000).
    Mede o custo dos serviços públicos na comunidade terrestre.
    As taxas de imposto nominais foram corrigidas pelos índices de avaliação locais para gerar o valor total da taxa de imposto para cada cidade.
    Diferenças intramunicipais na taxa de avaliação eram difíceis de obter e, portanto, não eram usadas.
    O coeficiente desta variável deve ser negativo.
    Será representada como **Imposto**.

11. PTRATIO: Proporção aluno-professor por distrito escolar da cidade.
    Mede os benefícios do setor público em cada cidade.
    A relação do rácio aluno-professor com a qualidade da escola não é totalmente clara, embora um rácio baixo deva significar que cada aluno recebe mais atenção individual.
    Esperamos o sinal em PTRATIO seja negativo.
    Será representada como **Prop. Prof.-Aluno**.

12. B: O resultado da equação $B=1000(Bk - 0,63)^2$ onde $Bk$ é a proporção de negros por bairro.
    Em níveis baixos a moderados de B, um aumento em B deve ter uma influência negativa no valor da habitação se os negros forem considerados vizinhos indesejáveis pelos brancos.
    No entanto, a discriminação de mercado significa que os valores das moradias são mais altos em níveis muito altos de B.
    Espera-se, portanto, uma relação parabólica entre a proporção de negros em um bairro e os valores das moradias.
    Será representada por **Prop. Negros/bairro**.

13. LSTAT: Proporção da população de "classe baixa", ou seja, com status inferior = 1/2 (proporção de adultos sem nível de ensino médio e proporção de trabalhadores do sexo masculino classificados como trabalhadores).
    A especificação logarítmica implica que as distinções de status socioeconômico significam mais nas camadas superiores da sociedade do que nas classes inferiores.
    Será representada por **Pop. Classe Baixa**.

### Variável de Saída (Resposta):

-   Valor do Imóvel: Valor médio de residências ocupadas pelo proprietário em US\$1.000 \[Milhares de dólares - k\$\].

### Fonte

StatLib - Carnegie Mellon University

# Objetivo

Este projeto tem o objetivo de descrever o comportamento dos preços dos imóveis em Boston sob três óticas:
:::{.incremental}
i)    Regressão Linear Simples - utilização de uma variável preditora para descrever o comportamento dos preços dos imóveis em Boston;
ii)   Regressão Linear Múltipla Parcial - utilização de duas variáveis uma quantitativa contínua e uma categórica para descrever o comportamento dos preços dos imóveis em Boston;
iii)  Regressão Linear Múltipla - utilização de um subconjunto de variáveis para descrever o comportamento dos preços dos imóveis em Boston.
:::

# Análise de Regressão Linear Simples

## Resultados

### Análise Descritiva

De modo a conhecer melhor o banco de dados analisado é importante realizar uma análise descritiva das variáveis que o compõem.
Na Tabela 1 pode se ver as medidas de resumo de posição e de tendência central destas variáveis.

```{r tab1:medidas_resumo}
#| echo: false
#| warning: false

# Medidas Resumo ----
summarytools::st_options(lang = "pt")

dados|>
  # select(-b, - rad, -zn)|>
  rename(
    "Idade do Imóvel" = age, "Índice Criminalidade" = crim, "Dist. Empregos" = dis, "Área Industrial" = indus,
    "Pop. Classe Baixa" = lstat, "Índice Oxido Nítrico" = nox, "Prop. Prof.-Aluno" = ptratio,
    "N° Cômodos" = rm, "Imposto Propriedade" = tax, "Valor do Imóvel" = medv, "Acessibilidade Rodovias" = rad, "Prop. Terreno Zoneado" = zn, "Prop. Negros/bairro" = b
  )|>
  summarytools::descr(
    stats = c("min", "q1", "med", "mean","q3", "max",  "sd", "cv", "Skewness", "Kurtosis"),
    justify = "c",
    style = "rmarkdown",
    transpose = T
  )|>
    kbl(
    caption = "Medidas Resumo dos dados",
    digits = 2,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", row.names = T, booktabs = T
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "scale_down", "repeat_header")
  )|>
  column_spec(1, bold = T
              )|>
  kable_material()

```

Para facilitar a compreenção das medidas apresentadas na Tabela 1, a Figura 1 mostra graficamente estas distribuições.

```{r fig1:Histograma}
#| echo: false
#| warning: false
#| fig-height: 9
#| fig-width: 7

# Histograma ----

## g1 age ----
g1 <- dados|>
  ggplot() +
  aes(x = age) +
  geom_histogram(
    aes(y = ..density..),
    bins = 40,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Unidades constuídas antes \nde 1940",
    x = "Proporção",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g2 crim ----
g2 <- dados|>
  ggplot() +
  aes(x = crim) +
  geom_histogram(
    aes(y = ..density..),
    fill = "#A6CEE3",
    colour = "#1F78B4",
    # binwidth = 2
    bins = 45
    ) +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Índice de Criminalidade",
    x = "Índice",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g3 dis ----
g3 <- dados|>
  ggplot() +
  aes(x = dis) +
  geom_histogram(
    aes(y = ..density..),
    bins = 45,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_x_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Distância para cinco centros \nde emprego.",
    x = "Distâncias Ponderadas",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g4 indus ----
g4 <- dados|>
  ggplot() +
  aes(x = indus) +
  geom_histogram(
    aes(y = ..density..),
    bins = 40,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour ="#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Negócios não varejistas \npor bairro",
    x = "Proporção de hectares ocupados",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g5 lstat ----
g5 <- dados|>
  ggplot() +
  aes(x = lstat) +
  geom_histogram(
    aes(y = ..density..),
    bins = 40,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = 'População de "classe baixa"',
    x = 'Percentual',
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g6 medv ----
g6 <- dados|>
  ggplot() +
  aes(x = medv) +
  geom_histogram(
    aes(y = ..density..),
    bins = 35,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Valor médio de residências \nocupadas",
    x = "Valor Médio",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g7 nox ----
g7 <- dados|>
  ggplot() +
  aes(x = nox) +
  geom_histogram(
    aes(y = ..density..),
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Concentração de Óxidos \nNitricos (NO)",
    x = "Partes por 10 milhões",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g8 ptratio ----
g8 <- dados|>
  ggplot() +
  aes(x = ptratio) +
  geom_histogram(
    aes(y = ..density..),
    bins = 30,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_x_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Aluno/Professor por bairro",
    x = "Proporção",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g9 rm ----
g9 <- dados|>
  ggplot() +
  aes(x = rm) +
  geom_histogram(
    aes(y = ..density..),
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Número médio de cômodos por \nhabitação",
    x = "Quantidade",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g10 tax ----
g10 <- dados|>
  ggplot() +
  aes(x = tax) +
  geom_histogram(
    aes(y = ..density..),
    bins = 35,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Taxa de imposto predial",
    x = "Valor por $10.000",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g11 zn ----
g11 <- dados|>
  ggplot() +
  aes(x = zn) +
  geom_histogram(
    aes(y = ..density..),
    bins = 35,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Proporção de Terreno Zoneado",
    x = "Proporção de Terreno",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## g12 b ----
g12 <- dados|>
  ggplot() +
  aes(x = b) +
  geom_histogram(
    aes(y = ..density..),
    bins = 35,
    fill = "#A6CEE3",
    colour = "#1F78B4") +
  geom_density(
    alpha = 0.2,
    fill = "#1F78B4",
    colour = "#1F78B4") +
  scale_y_continuous(
    labels = scales::number_format(
      big.mark = ".",
      decimal.mark = ","
    )) +
  labs(
    title = "Proporção de Negros por bairro",
    x = "Proporção",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

g1 + g2 + g3 + g4 + g5 + g6 + g7 + g8 + g9 + g10 + g11 + g12 + 
  plot_layout(ncol = 3) + 
  plot_annotation(
    title = "Figura 1: Histogramas das variáveis em análise.",
    caption = "Fonte: StatLib - Carnegie Mellon University",
    tag_levels = c("A", "1"), tag_sep = ".")&
  theme(
    plot.tag.position = c(0.9, 0.8),
    plot.tag = element_text(size = 12, hjust = 0.5, vjust = 1))

```

Desta análise inicial, destancam-se algumas características:

-   O Índice de criminalidade per capita por bairro é bastante baixo na maioria dos bairros (Figura 1B);
-   A Proporção de terreno residencial zoneada para lotes acima de 25.000 sq.ft. contém uma alta concentração de valores zeros (Figura 1K);
-   Verifica-se uma concentração de empresas com cerca de 18 hectares em diversos bairros (Figura 1D);
-   Há uma concentração de imóveis com alto valor total do imposto predial (Figura 1J);
-   A maior parte dos bairros tinha alta proporção de negros (Figura 1L).

Tendo em vista o fato de as variáveis categóricas não estarem representadas na Figura 1, foi construída a Figura 2 com gráficos que possibilitam a avaliação do comportamento dessas variáveis de maneira mais adequada.

```{r fig2:Grafico_Barras+Rosca}
#| echo: false
#| warning: false
#| fig-height: 6
#| fig-width: 7

p1 <- dados|>
  ggplot(aes(x = rad, y = after_stat(count)/sum(after_stat(count))))+
  geom_bar(fill = "#1F78B4")+ 
  geom_text(
    aes(
      label = scales::percent((after_stat(count))/sum(after_stat(count)), big.mark = ".", decimal.mark = ","),
      y = after_stat(count)/sum(after_stat(count))),
    stat = "count", vjust = -0.2, size = 1.8
  ) +
  labs(
    title = "Índice de Acessibilidade às Rodovias",
    x = "Índice",
    y = "Frequência Relativa"
  )+
  scale_y_continuous(labels = scales::percent)+
  scale_x_discrete(limits = c(1:25))+
  theme_minimal()+
    theme(
    legend.position = "none", title = element_text(size = 7.5))

p2 <- dados %>% 
  count(chas) %>%
  mutate(
    tipo = case_when(
      chas == "0" ~ "Não Margeiam o rio",
      chas == "1" ~ "Margeiam o rio"),
    pct = round(prop.table(n)*100, 2), 
    rotulo = glue::glue('{tipo}\n{n} ({pct}%)')) %>% 
  ggpubr::ggdonutchart(., "pct", 
                       label = "rotulo", lab.pos = "out",
                       lab.font = c(3, "plain", "black"),
                       fill = "chas",  color = "white",
                       palette = c("#1F78B4", "#A6CEE3"))+
  labs(
    title = "Imóveis que margeiam o Rio Charles"
  )+
  theme(
    legend.position = "none", title = element_text(size = 7.5)
  )

p1 + p2 +
  plot_layout(nrow = 2, widths = 3) +
  plot_annotation(
    title = "Figura 2: Distribuição de Frequência das Variáveis Categóricas.",
    caption = "Fonte: StatLib - Carnegie Mellon University",
    tag_levels = c("A", "1"), tag_sep = ".") &
  theme(
    plot.tag.position = c(0.9, 0.8),
    plot.tag = element_text(size = 12, hjust = 0, vjust = -0.4))

```

Avaliando a Figura 2 é possível constatar na Figura 2A que representa a frequência do Índice de Acessibilidade às Rodovias a existência de um comportamento tri modal e que há uma lacuna entre os índices, sendo este *gap* entre o índice 8 e o índice 24 indicando assim que há uma concentração de imóveis próximos a acessos a rodovias e que há uma parcela, cerca de 26%, que estão distantes desses acessos, podendo pressupor que esses imóveis são desvalorizados em relação aos mais próximos.

A Figura 2B que retrata os Imóveis que margeiam o Rio Charles é possível identificar que mais de 93% não margeiam o rio, apenas uma pequena parcela margeia, podendo pressupor uma maior valorização dos imóveis que margeiam o rio.

### Análise de Dados Atípicos

Com base na variável que indica se o imóvel margeia ou não o Charles River, pode-se realizar a análise de dispersão dos dados por meio de gráficos do tipo BoxPlot, como se vê na Figura 3.

```{r fig3:BoxPlot}
#| echo: false
#| warning: false
#| fig-height: 9
#| fig-width: 7

# BoxPlot ----
{
## b1 age ----
b1 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = age)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Unidades constuídas antes \nde 1940",
    x = "Posição",
    y = "Proporção antes de 1940"
  )+theme_minimal(base_size = 7.5)

## b2 crim ----
b2 <- dados|>
  mutate(
    chas = lvls_revalue(chas, c("Na Margem", "Afastado"))
  )|>
  ggplot(aes(x = chas, y = crim)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Índice de Criminalidade",
    x = "Posição",
    y = "Índice"
  )+theme_minimal(base_size = 7.5)

## b3 dis ----
b3 <- dados|>
  mutate( chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = dis)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Distância para cinco centros \nde emprego.",
    x = "Posição",
    y = "Distâncias Ponderadas"
  ) +
  scale_y_continuous(
    labels = scales::number_format(
      dig.mark = ".",
      decimal.mark = ",")
    )+theme_minimal(base_size = 7.5)
      
## b4 indus ----
b4 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = indus)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Negócios não varejistas \npor bairro",
    x = "Proporção de hectares ocupados",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## b5 lstat ----
b5 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = lstat)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = 'População de "classe baixa"',
    x = "Posição",
    y = "Percentual"
  )+theme_minimal(base_size = 7.5)

## b6 medv ----
b6 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = medv)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Valor médio de residências \nocupadas",
    x = "Posição",
    y = "Valor médio"
  )+theme_minimal(base_size = 7.5)

## b7 nox ----
b7 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = nox)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Concentração de Óxidos \nNitricos (NO)",
    x = "Posição",
    y = "Partes por 10 milhões"
  ) +
  scale_y_continuous(
    labels = scales::number_format(
      dig.mark = ".",
      decimal.mark = ",")
    )+theme_minimal(base_size = 7.5)

## b8 ptratio ----
b8 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = ptratio)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Aluno/Professor por bairro",
    x = "Posição",
    y = "Proporção"
  ) +
  scale_y_continuous(
    labels = scales::number_format(
      dig.mark = ".",
      decimal.mark = ",")
    )+theme_minimal(base_size = 7.5)

## b9 rm ----
b9 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = rm)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Número médio de cômodos por \nhabitação",
    x = "Posição",
    y = "Quantidade"
  )+theme_minimal(base_size = 7.5)

## b10 tax ---- 
b10 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = tax)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Taxa de imposto predial",
    x = "Posição",
    y = "Valor por $10.000"
  )+theme_minimal(base_size = 7.5)

## b11 zn ---- 
b11 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = zn)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Proporção de Terreno Zoneado",
    x = "Proporção de Terreno",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

## b12 b ---- 
b12 <- dados|>
  mutate(chas = lvls_revalue(chas, c("Na Margem", "Afastado")))|>
  ggplot(aes(x = chas, y = b)) +
  geom_boxplot(col="#1F78B4", fill="#A6CEE3", alpha = 0.5)+
  labs(
    title = "Proporção de Negros por bairro",
    x = "Proporção",
    y = "Densidade"
  )+theme_minimal(base_size = 7.5)

b1 + b2 + b3 + b4 + b5 + b6 + b7 + b8 + b9 + b10 + b11 + b12 +  
  plot_layout(ncol = 3) + 
  plot_annotation(
    title = "Figura 3: BoxPlot das variáveis em análise.",
    caption = "Fonte: StatLib - Carnegie Mellon University",
    # theme = theme_minimal(plot.title = element_text(size = 10)),
    tag_levels = c("A", "1"), tag_sep = ".") &
  theme(
    plot.tag.position = c(0.9, 0.8),
    plot.tag = element_text(size = 12, hjust = 0, vjust = -0.4))
}

```

Pode-se verificar pela Figura 3 que cerca de 2/3 das variáveis apresentam valores atípicos (*outliers*), entretanto o tratamento destes *outliers* em todos os casos parece, salvo melhor juízo, ser o mesmo.
Observa-se que há coerência entre eles, isto é, são realizações possíveis e não devem ser desprezadas como se fossem erros ou dados irrelevantes.
Isto se deve a tremenda variedade em tipos, propósitos e status dos imóveis avaliados.
Esses dados por sua vez, representam um maior desafio ao modelamento a que esse trabalho se propõe.

### Relação entre as variáveis

Antes da proposição do modelo de regressão mais bem elaborado é conveniente uma avaliação gráfica da dispersão dos valores das variáveis em relação à variável resposta **Valor Médio do Imóvel**.
A Figura 4 apresenta essas dispersões de pontos e já apresenta uma linha de tendência para os valores observados.

```{r fig4:regressao}
#| echo: false
#| warning: false
#| fig-height: 8
#| fig-width: 7

# Dispersão ----
{
  ## d1 age ----
  d1 <- dados|>
    ggplot(aes(y = medv, x = age)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Unidades constuídas antes de \n1940",
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção antes de 1940'
    )+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 5, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    scale_y_continuous(
      labels = scales::number_format(
        big.mark = ".",
        decimal.mark = ","
      ))+
    geom_smooth(method=lm, se=T, color="tomato", size = 0.6)+
    theme_minimal(base_size = 7.5)
  
  ## d2 crim ----
  d2 <- dados |>
    ggplot(aes(
      y = medv, 
      x = crim)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = 'Índice de Criminalidade',
      y = 'Valor Médio (por $1.000)',
      x = 'Índice'
    )+
    geom_smooth(method=lm, se=T, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 50, label.y = 50, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)

  ## d3 dis ----
  d3 <- dados |>
    ggplot(aes(
      y = medv, 
      x = dis)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Distância para cinco centros de \nemprego.",
      y = 'Valor Médio (por $1.000)',
      x = 'Distâncias Ponderadas'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 9, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    scale_x_continuous(
      labels = scales::number_format(
        big.mark = ".",
        decimal.mark = ","
      ))+
    theme_minimal(base_size = 7.5)
  
  ## d4 indus ----
  d4 <- dados |>
    ggplot(aes(
      y = medv, 
      x = indus)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Negócios não varejistas por \nbairro",
      y = 'Valor Médio (por $1.000)',
      x = 'Hectares Ocupados'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 1.5, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)

  ## d5 lstat ----
  d5 <- dados |>
    ggplot(aes(
      y = medv, 
      x = lstat)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = 'População de "classe baixa"',
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 25, label.y = 47.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)

  ## d6 nox ----
  d6 <- dados |>
    ggplot(aes(
      y = medv, 
      x = nox)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Concentração de Óxidos \nNitricos (NO)",
      y = 'Valor Médio (por $1.000)',
      x = 'Partes por 10 milhões'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 0.75, label.y = 47.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    scale_x_continuous(
      labels = scales::number_format(
        big.mark = ".",
        decimal.mark = ","
      ))+
    theme_minimal(base_size = 7.5)
  
  ## d7 ptratio ----
  d7 <- dados |>
    ggplot(aes(
      y = medv, 
      x = ptratio)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Aluno/Professor por bairro",
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 13, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    scale_x_continuous(
      labels = scales::number_format(
        big.mark = ".",
        decimal.mark = ","
      ))+
    theme_minimal(base_size = 7.5)
  
  ## d8 rm ----
  d8 <- dados |>
    ggplot(aes(
      y = medv, 
      x = rm)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Número médio de cômodos por \nhabitação",
      y = 'Valor Médio (por $1.000)',
      x = 'Quantidade'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 3.7, label.y = 45, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)
  
  ## d9 tax ----
  d9 <- dados |>
    ggplot(aes(
      x = tax, y = medv)) +
    geom_point(color = "#1F78B4")+
    labs(
      title = "Taxa de imposto predial",
      y = 'Valor Médio (por $1.000)',
      x = 'Valor por $10.000'
    )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 200, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)
  
  ## d10 zn ----
  d10 <- dados |>
    ggplot(aes(
      x = zn, y = medv)) +
    geom_point(color = "#1F78B4")+
  labs(
    title = "Proporção de Terreno Zoneado",
    x = "Proporção",
    y = "Densidade"
  )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 20, label.y = 7.5, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)
  
  ## d11 b ----
  d11 <- dados |>
    ggplot(aes(
      x = b, y = medv)) +
    geom_point(color = "#1F78B4")+
  labs(
    title = "Proporção de Negros por bairro",
    x = "Proporção",
    y = "Densidade"
  )+
    geom_smooth(method=lm, se=TRUE, color="tomato", size = 0.6)+
    ggpubr::stat_cor(
      aes(label = ..r.label..),
      cor.coef.name = c("rho"),
      label.sep = "; ", geom = "text",
      color="tomato", method = "pearson", 
      label.x = 50, label.y = 45, show.legend = F,
      p.accuracy = 0.001, r.accuracy = 0.0001,
      size = 2)+
    theme_minimal(base_size = 7.5)

  d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9 + d10 + d11 +  
    plot_layout(ncol = 3) + 
    plot_annotation(
      title = "Figura 4: Reta de regressão ajustada entre o Valor médio dos imóveis e \ndemais medições",
      tag_levels = c("A", "1"), tag_sep = ".") &
    theme(
      legend.position = "none",
      plot.tag.position = c(0.9, 0.8),
      plot.tag = element_text(size = 12, hjust = 0, vjust = -0.4))

}
```

Para avaliar a significância das correlações entre as variáveis com relação ao **Valor Médio do Imóvel** segue a Tabela 2 com os resultados do Teste de Hipóteses com nível de significância de 5% que tem como hipóteses:

$$H_0: \widehat{\rho} = 0$$

$$H_1: \widehat{\rho} \neq 0.$$

```{r tab2:TesteHipo_CorPearson}
#| echo: false
#| warning: false

# Correlação 2 ----

# Cor Test
cortestCrim <- stats::cor.test(dados$medv, dados$crim)
cortestZn <- stats::cor.test(dados$medv, dados$zn)
cortestIndus <- stats::cor.test(dados$medv, dados$indus)
cortestNox <- stats::cor.test(dados$medv, dados$nox)
cortestRm <- stats::cor.test(dados$medv, dados$rm)
cortestAge <- stats::cor.test(dados$medv, dados$age)
cortestDis <- stats::cor.test(dados$medv, dados$dis)
cortestTax <- stats::cor.test(dados$medv, dados$tax)
cortestPtratio <- stats::cor.test(dados$medv, dados$ptratio)
cortestB <- stats::cor.test(dados$medv, dados$b)
cortestLstat <- stats::cor.test(dados$medv, dados$lstat)

# Estatística t
resultados <- rbind(cortestCrim$statistic, 
           cortestZn$statistic, 
           cortestIndus$statistic,
            cortestNox$statistic,
            cortestRm$statistic,
            cortestAge$statistic,
            cortestDis$statistic,
            cortestTax$statistic,
            cortestPtratio$statistic,
            cortestB$statistic,
            cortestLstat$statistic)

# p-valor
aux <- rbind(cortestCrim$p.value,
cortestZn$p.value,
cortestIndus$p.value,
cortestNox$p.value,
cortestRm$p.value,
cortestAge$p.value,
cortestDis$p.value,
cortestTax$p.value,
cortestPtratio$p.value,
cortestB$p.value,
cortestLstat$p.value)

resultados <- cbind(resultados, aux)

# IC
aux <- rbind(cortestCrim$conf.int[1:2],
             cortestZn$conf.int[1:2],
             cortestIndus$conf.int[1:2],
             cortestNox$conf.int[1:2],
             cortestRm$conf.int[1:2],
             cortestAge$conf.int[1:2],
             cortestDis$conf.int[1:2],
             cortestTax$conf.int[1:2],
             cortestPtratio$conf.int[1:2],
             cortestB$conf.int[1:2],
             cortestLstat$conf.int[1:2])

resultados <- cbind(resultados, aux)
resultados <- round(resultados,3)
resultados[,2] <- "<0,0001"
rownames(resultados) <- c("Índice Criminalidade", "Prop. Terreno Zoneado", "Área Industrial", "Índice Oxido Nítrico", "N° Cômodos", "Idade do Imóvel", "Dist. Empregos", "Imposto Propriedade", "Prop. Prof.-Aluno", "Prop. Negros/bairro", "Pop. Classe Baixa")
colnames(resultados) <- c("t", "p-valor", "LI", "LS")

resultados|>
  kbl(
    caption = "Teste de Hipótese para Correlação",
    digits = 5,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", row.names = T, booktabs = T
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T
  )|>
  footnote(
    general = "Teste realizado com 5% de significância",
    general_title = "Nota:",
    footnote_as_chunk = T
  )|>
  kable_material()


```

Conforme expresso na Tabela 2, levando em consideração o **p-valor** a Hipótese Nula foi rejeitada, e com 95% de confiança se pode afirmar que **é significativa a relação linear entre todas as variáveis em estudo.**

Na avaliação da Figura 4, observa-se que nenhuma das variáveis tem uma forte correlação com o valor médio dos imóveis.
A Tabela 3, apresenta os valores calculados de $\hat \beta_0$ e $\hat\beta_1$ que estimam os valores do modelo $Y_i =\beta_0 + \beta_1X_i + \epsilon_i$ com seus respectivos erros padrão ($\sigma_0$ e $\sigma_1$), além de calcular o p-valor desta regressão linear como forma de identificar a rejeição ou não do modelo proposto.
Nesta mesma linha, o valor estimado do Coeficiente de Determinação ($R^2$) também foi calculado.

```{r ajuste_dos_modelos}
#| echo: false
#| warning: false

# Ajuste do Modelo 2 ----

mCrim <- lm(dados$medv~dados$crim)
mIndus <- lm(dados$medv~dados$indus)
mNox <- lm(dados$medv~dados$nox)
mRm <- lm(dados$medv~dados$rm)
mAge <- lm(dados$medv~dados$age)
mDis <- lm(dados$medv~dados$dis)
mTax <- lm(dados$medv~dados$tax)
mPtratio <- lm(dados$medv~dados$ptratio)
mLstat <- lm(dados$medv~dados$lstat)
mZn <- lm(dados$medv~dados$zn)
mB <- lm(dados$medv~dados$b)

# Calculando e armazenando o beta0 e erro padrão0
resultados <-  rbind(
  summary(mCrim)$coefficients[1,],
  summary(mIndus)$coefficients[1,],
  summary(mNox)$coefficients[1,],
  summary(mRm)$coefficients[1,],
  summary(mAge)$coefficients[1,],
  summary(mDis)$coefficients[1,],
  summary(mTax)$coefficients[1,],
  summary(mPtratio)$coefficients[1,],
  summary(mLstat)$coefficients[1,],
  summary(mZn)$coefficients[1,],
  summary(mB)$coefficients[1,])

# Removendo testes
resultados <-  resultados[, -c(3)]

# Calculando e armazenando o beta1 e erro padrão1
aux <-  rbind(
  summary(mCrim)$coefficients[2,],
  summary(mIndus)$coefficients[2,],
  summary(mNox)$coefficients[2,],
  summary(mRm)$coefficients[2,],
  summary(mAge)$coefficients[2,],
  summary(mDis)$coefficients[2,],
  summary(mTax)$coefficients[2,],
  summary(mPtratio)$coefficients[2,],
  summary(mLstat)$coefficients[2,],
  summary(mZn)$coefficients[2,],
  summary(mB)$coefficients[2,])

# Mantém apenas beta1 e o erro padrão
aux <- aux[, -c(3)]

resultados <- cbind(resultados, aux)

# Função para calcular o p-valor
# lmp <- function (modelobject) {
#   if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
#   f <- summary(modelobject)$fstatistic
#   p <- pf(f[1],f[2],f[3],lower.tail=F)
#   attributes(p) <- NULL
#   return(p)
# }
# 
# # Calculando e armazenando o p-valor
# aux <- rbind(
#   lmp(mCrim), lmp(mIndus),
#   lmp(mNox), lmp(mRm), lmp(mAge), 
#   lmp(mDis), lmp(mTax), lmp(mPtratio),
#   lmp(mLstat), lmp(mZn),lmp(mB)
# )

# resultados <- cbind(resultados, aux)

# Calculando e armazenando o Coeficiente de Correlação
aux <-  rbind(
  summary(mCrim)$r.squared,
  summary(mIndus)$r.squared,
  summary(mNox)$r.squared,
  summary(mRm)$r.squared,
  summary(mAge)$r.squared,
  summary(mDis)$r.squared,
  summary(mTax)$r.squared,
  summary(mPtratio)$r.squared,
  summary(mLstat)$r.squared,
  summary(mZn)$r.squared,
  summary(mB)$r.squared)

resultados <- cbind(resultados, aux)
resultados <- round(resultados,3)
resultados[,3] <- "<0,0001"
resultados[,6] <- "<0,0001"
# Inserindo o nome das variáveis (colunas)
rownames(resultados) <- c("Índice Criminalidade", "Área Industrial", "Índice Oxido Nítrico", "N° Cômodos", "Idade do Imóvel", "Dist. Empregos", "Imposto Propriedade", "Prop. Prof.-Aluno", "Pop. Classe Baixa", "Prop. Terreno Zoneado", "Prop. Negros/bairro")

# "Valor do Imóvel" = medv, "Acessibilidade Rodovias" = rad,  = zn,  = b

# Inserindo o nome das linhas
# colnames(resultados) <- c("$\\beta_0$", "$\\sigma_0$", "$\\beta_1$", "$\\sigma_1$", "p-valor", "$R^2$")

```

```{r tab3:modelos_ajustados}
#| echo: false
#| warning: false

# resultados|>
#   kbl(
#     caption = "Valores dos modelos de regressão linear simples.",
#     format.args=list(big.mark=".", decimal.mark=","),
#     digits = 3, align = "c", row.names = T, booktabs = T,
#     escape = FALSE,
#   )|>
#   kable_styling(
#     full_width = F, position = 'center', 
#     latex_options = c("striped", "HOLD_position", "repeat_header")
#   )|>
#   column_spec(1, bold = T
#   )|>
#   kable_material()


resultados|>
  kbl(
    caption = "Sumarização dos Modelos Ajustados de Regressão Linear Simples - RLS.",
    format.args=list(big.mark=".", decimal.mark=","),
    digits = 3, align = "c", row.names = T, booktabs = T,
    escape = FALSE,
    col.names = c("Estimativa", "Erro Padrão", "p-valor", "Estimativa", "Erro Padrão", "p-valor", "$R^2$")
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header", "scale_down")
  )|>
  column_spec(1, bold = T
  )|>
  add_header_above(c(" " = 1, "Sumarização para Beta 0" = 3, "Sumarização para Beta 1" = 3, " " = 1))|>
  kable_material()


```

### Interpretação dos Parâmetros dos Modelos Ajustados

Baseado na análise dos gráficos de dispersão e considerando os modelos ajustados expressos na Tabela 3 é possível constatar que tanto $\hat{\beta_0}$ quanto $\hat{\beta_1}$ são significantes para todos os modelos ajustados, com base no p-valor, levando em consideração a ordem de precedência das variáveis na referida tabela, seguem as interpretações com base nos parâmetros estimados:

-   Modelo que avalia o Índice de Criminalidade:
    -   Para cada **valorização** de 0,415 no Índice de Criminalidade o Valor Médio dos Imóveis decresce em \$24 033,00 ou seja, os imóveis se desvalorizam em regiões cujo Índice de Criminalidade é elevado.
-   Modelo que avalia a Área Industrial:
    -   O Valor Médio dos Imóveis decresce em \$29 755,00 para cada **valorização** proporcional de 0,648 hectares de Negócios não Varejistas.
-   Modelo que avalia o Índice Oxido Nítrico:
    -   Há uma **desvalorização** de cerca de \$41 346,00 no Valor Médio dos Imóveis para cada aumento de 33 916 pphm (partes por 100 milhões) no Índice Oxido Nítrico, ou seja, há uma desvalorização no valor dos imóveis que estão situados em regiões cujo ar é mais poluída.
-   Modelo que avalia o N° Cômodos:
    -   Há um **valorização** de cerca de \$34 671,00 no Valor Médio dos Imóveis para cada aumento de aproximadamente 9 cômodos, ou seja, os imóveis são mais valorizados a medida que possuem mais cômodos.
-   Modelo que avalia a Idade do Imóvel:
    -   Há uma **desvalorização** de cerca de \$30 979,00 para cada aumento proporcional de 0,123 unidades próprias construídas antes de 1940, ou seja, os imóveis são desvalorizados à medida que são mais antigos.
-   Modelo que avalia a Dist. Empregos:
    -   Há um **valorização** de cerca de \$18 390,00 no Valor Médio dos Imóveis à medida que a distância para os centros de emprego na região de Boston aumenta.
-   Modelo que avalia o Imposto de Propriedade:
    -   Há uma **desvalorização** de cerca de \$32 971,00 no Valor Médio dos Imóveis à medida que há um aumento de aproximadamente 0,026 no valor proporcional total do Imposto de Propriedade, ou seja, quanto maior o imposto pago na região, maior a desvalorização do imóvel.
-   Modelo que avalia a Prop. Prof.-Aluno:
    -   Há uma **desvalorização** de cerca de \$62 345,00 no Valor Médio dos Imóveis à medida que há um aumento de aproximadamente 2,157 na proporção professor aluno. Como a própria descrição da variável descreve como confusa essa relação, de fato se mostra, com base no modelo ajustado da variável, pois demostra que os imóveis se desvalorizam a medida que os benefícios do setor público aumentam!
-   Modelo que avalia a Pop. Classe Baixa:
    -   Há uma **desvalorização** de cerca de \$34 554,00 no Valor Médio dos Imóveis à medida que há um aumento de aproximadamente 0,950 na proporção da Pop. de Classe Baixa, ou seja, os imóveis se desvalorizam a medida que a classe social dos habitantes da região cai.
-   Modelo que avalia a Prop. Terreno Zoneado:
    -   Há um **valorização** de cerca de \$20 918,00 no Valor Médio dos Imóveis à medida que há um aumento de aproximadamente 0,142 na Prop. Terreno Zoneado, ou seja, os imóveis são valorizados em regiões com maior proporção de zoneamento.
-   Modelo que avalia a Prop. Negros/bairro:
    -   Há um **valorização** de cerca de \$10 551,00 no Valor Médio dos Imóveis à medida que há um aumento de aproximadamente 0,034 na proporção de Prop. Negros/bairro, ou seja, os imóveis são mais valorizados em regiões cuja proporção de negros é maior.

Tendo em vista que nesse primeiro momento a proposta é a implementação de técnicas de Regressão Linear Simples - RLS, a escolha de uma variável explicativa que aparenta melhor possibilidade de explicação do Preço Médio dos Imóveis se faz necessária.
Após análise dos gráficos de dispersão, Coeficiente de Correlação e avaliação dos Coeficientes de Determinação a variável escolhida foi **Pop. Classe Baixa**, logo as análises a seguir serão direcionadas a avaliar o modelo com esta variável.

### Significância do Modelo

Tendo em vista a necessidade de se avaliar a significância dos parâmetros, o teste de hipótese para tal situação será realizado, contendo as seguintes hipóteses:

$$H_0: \hat{\beta_0} = 0$$ $$H_1: \hat{\beta_0} \neq 0.$$

As Tabelas 4 e 5 trazem os principais resultados da tabela ANOVA e do Intervalo de Confiança para os parâmetros, possibilitando assim inferir sobre o modelo ajustado.

```{r tab4:anova_ic}
#| echo: false
#| warning: false

mLstat <- lm(dados$medv~dados$lstat)

fit_anova <- anova(mLstat)
ic_parametros <- confint(mLstat)

resultados <- cbind(fit_anova, ic_parametros)

rownames(resultados) <- c("β0", "β1")

rownames(ic_parametros) <- c("β0", "β1")

colnames(resultados) = c("GL", "Soma de \nQuadrados", "Quadrado Médio", "Valor F-Snedecor", "p-valor", "2,5%", "97,5%")

# resultados

# resultados %>%
#   tibble::as_tibble() %>% 
#   kableExtra::kbl(
#     caption = "Análise de Variância (ANOVA) e Intervalos de Confiança \npara os parâmetros estimados no MRLS.",
#     format.args=list(big.mark=".", decimal.mark=","),
#     digits = 3, align = "c", row.names = T, booktabs = T,
#     escape = F
#     # col.names = c("GL", "Soma de Quadrados", "Quadrado Médio", "Estatística F-Snedecor", "p-valor", "α = 2,5%", "(1 - α) = 97,5%")
#   ) %>% 
# kableExtra::kable_styling(
#   full_width = F, position = 'center',
#   latex_options = c("striped", "HOLD_position", "repeat_header", "scale_down")
# )
  # column_spec(1, bold = T)|>
  # kable_material()
  # add_header_above(c(" " = 1, "ANOVA" = 5, "Intervalos de Confiança" = 2), bold = T)|>

fit_anova <- round(fit_anova,2)
fit_anova[1,5] <- "<0,0001"
fit_anova[2,4] <- " "
fit_anova[2,5] <- " "
fit_anova <- cbind(c("Regressão","Resíduos"), fit_anova)
fit_anova %>%
  kbl(
    caption = "Resultados da ANOVA.",
    digits = 4,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", 
    row.names = F,
    col.names =
      c(" ", "GL", "Soma de Quadrados", "Quadrado Médio", "Valor F-Snedecor", "p-valor")
  ) %>%
  # footnote(
  #   number = c(
  #     "Linha 1: Dados referentes a β0", 
  #     "Linha 2: Dados referentes a β1",
  #     "GL: Graus de Liberdade", 
  #     "SQ: Soma de Quadrados", 
  #     "QM: Quadrado Médio", 
  #     "Estatística: F-Snedecor"
  #   ),
  #   number_title = "Legenda:",
  #   footnote_as_chunk = F
  # )|>
  
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = F
  )|>
  kable_material()
ic_parametros <- round(ic_parametros,3)
ic_parametros <- cbind(c("Beta 0","Beta 1"), ic_parametros)
ic_parametros %>% 
  kbl(
    caption = "Intervalo de Confiança.",
    digits = 3,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", 
    row.names = F,
    col.names =
      c("Parâmetro", "LI", "LS")
  ) %>%
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = F
  )|>
  kable_material()

```

A Tabela 4, que traz os resultados da tabela ANOVA, para o modelo que avalia o **Valor Médio dos Imóveis** em relação a **Pop. Classe Baixa**, corrobora com a significância do $\hat{\beta_1}$, pois sendo o p-valor menor que o nível de significância ($\alpha = 5\%$) possibilita rejeitar $H_0$, indicando ser significante para o modelo.
O Intervalo de Confiança para os parâmetros estimados, Tabela 5, mostra que **com 95% de confiança é possível afirmar que o verdadeiro valor de** $\hat{\beta_0}$ está entre `r glue::glue('({scales::number(confint(mLstat)[1,1], accuracy = 0.0001, big.mark = ".", decimal.mark = ",")}; {scales::number(confint(mLstat)[1,2], accuracy = 0.0001, big.mark = ".", decimal.mark = ",")})')` e que o verdadeiro valor de $\hat{\beta_1}$ está entre `r glue::glue('({scales::number(confint(mLstat)[2,1], accuracy = 0.0001, big.mark = ".", decimal.mark = ",")}; {scales::number(confint(mLstat)[2,2], accuracy = 0.0001, big.mark = ".", decimal.mark = ",")})')`.

### Análise de Resíduos

Sendo de fundamental importância para a verificação da bondade do modelo, a análise de resíduos possibilita avaliar se refletem o comportamento do modelo, para tanto se construiu a Figura 5 para iniciar as análises descritas.

```{r fig5:analise_residuos}
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 6

mLstat <- lm(medv~lstat, data = dados)

dados_mLstat_resid <- broom::augment(mLstat)

# Gráfico de Resíduos contra Valor Médio
d1 <- dados_mLstat_resid|>
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(color = "#1F78B4") +
  geom_hline(yintercept = 0, linetype = 2, size = 0.2) +
  geom_smooth(
    se = T, color = "tomato", method = 'loess', 
    size = 0.5, formula = 'y ~ x')+
  labs(
    x = "Valores Médios Ajustados",
    y = "Resíduos Ordinários",
    title = "Linearidade"
  )+
  scale_x_continuous(breaks = seq(0,30,5))+
  theme_minimal(base_size = 7.5)+
  theme(legend.position = "none")

## Gráfico de normalidade dos resíduos
d2 <- dados_mLstat_resid %>% 
  ggplot(aes(sample = .std.resid)) + 
  qqplotr::stat_qq_band(alpha = 0.3) +
  qqplotr::stat_qq_point(color = "#1F78B4") +
  qqplotr::stat_qq_line(linetype = 1, size = 0.5, color = "tomato") +
  labs(
    x = "Quantil Teórico",
    y = "Quantil Amostral",
    title = "Normalidade dos Resíduos"
  )+
  scale_x_continuous(breaks = seq(-3,3,1))+
  theme_minimal(base_size = 7.5)

## Gráfico Homogeneidade de Variâncias (Locação-Escala)
d3 <- dados_mLstat_resid %>% 
  ggplot(aes(x = .fitted, y = sqrt(abs(.std.resid)))) + 
  geom_point(color = "#1F78B4") +
  geom_smooth(
    se = T, color = "tomato", method = 'loess', formula = 'y ~ x')+
  labs(
    x = "Valores Ajustados",
    y = "√|Resíduos Padronizados|",
    title = "Homogeneidade de Variâncias \n(Locação-Escala)"
  )+
  theme_minimal(base_size = 7.5)+
  theme(legend.position = "none")

d1 + d2 + d3 + 
  plot_layout(ncol = 2) +
  plot_annotation(
  title = "Figura 5: Análise de resíduos do modelo ajustado",
  tag_levels = c("A", "1"), tag_sep = ".") &
  theme(
    legend.position = "none",
    plot.tag.position = c(0.5, 0.8),
    plot.tag = element_text(size = 12, hjust = 0, vjust = -0.4)
  )

```

Analisando a Figura 5A, que traz o gráfico que avalia a **linearidade** do modelo se constata uma não aleatoriedade, possibilitando identificar um certo padrão, aparentando um afunilamento dos dados, indicando assim uma variância não constante (pela mudança da amplitude dos dados), em que mostra uma menor variabilidade dos resíduos no início e segue aumentando a medida que crescem os valores ajustados.

O gráfico que avalia a Normalidade dos Resíduos, Figura 5B, também não mostra um comportamento adequado para considerar o modelo como bom, pois é possível identificar que as caldas fogem e muito da reta de referência e do intervalo de confiança, inclusive, demonstrando haver pontos significativamente influentes que afetam o comportamento dos resíduos, logo, **se conclui que este não é um bom modelo para explicar o valor médio dos imóveis**.

O gráfico que avalia a Homogeneidade de Variâncias, Figura 5C, mostra que a Variância não é constante.

Ainda assim, para fins de implementação das técnicas apresndidas até o momento, serão realizados os **Testes de Diagnóstico** para avaliação dos resultados, com a expectativa dos mesmos correoborarem com as interpretações obtidas através da análise gráfica.

<!-- Os dados mostram que existem dois valores atipicos que impedem uma análise mais apurada da homocedasticidade quando observados os valores médios ajustados dos imóveis. Percebe-se ainda que apenas um grupo de dados da região central da distribuição de Student apresenta certa normalidade, enquanto as "caldas" da distribuição afastam-se significativamente da reta de normalidade. -->

### Testes de Diagnóstico

Serão realizados os Testes de Significância, como forma secundária de avaliação, sendo descritos os testes aplicados para fins didáticos, tendo em vista a conclusão obtida com as análises gráficas, sendo estes:

-   Normalidade:
    -   Teste de Kolmogorov-Smirnov
    -   Teste de Shapiro-Wilks
-   Homocedasticidade:
    -   Teste de Goldfeld-Quandt
    -   Teste de Breush-Pagan
    -   Teste de Park
-   Linearidae:
    -   Teste F para linearidade
-   Independência:
    -   Teste para avaliação da independência dos resíduos

Sendo estes uma forma secundária de avaliação:

<!-- Pode-se ainda utilizar um conjunto de testes de diagnóstico para confirmar este novo teste de significância. -->

<!-- Como: -->

<!-- - Teste de Kolmogorov-Smirnov -->

<!-- - Teste de Shapiro-Wilks -->

<!-- - Teste de Goldfeld-Quandt -->

<!-- - Teste de Breush-Pagan -->

<!-- - Teste de Park -->

<!-- - Teste F para linearidade -->

<!-- - Teste para avaliação da independência dos resíduos -->

#### Teste de Kolmogorov-Smirnov

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false


t.ks = ks.test(dados_mLstat_resid$.resid, "pnorm", mean(dados_mLstat_resid$.resid), sd(dados_mLstat_resid$.resid))

```

Avalia o grau de concordância entre a distribuição de um conjunto de valores observados e determinada distribuição teórica.
Consiste em comparar a distribuição de frequência acumulada da distribuição teórica com aquela observada.
Realizado o teste obteve-se um p-valor de aproximadamente `r round(t.ks[[2]][1],5)`, o que inviabiliza rejeitar a hipótese de que haja normalidade entre os dados, com um grau de confiabilidade minimamente razoável.

#### Teste de Shapiro-Wilks

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

t.sw = shapiro.test(dados_mLstat_resid$.resid)

```

O teste de Shapiro-Wilks é um procedimento alternativo ao teste de Kolmogorov-Smirnov para avaliar normalidade.
Realizado o teste obteve-se um p-valor de aproximadamente `r round(t.sw[[2]][1],5)`, o que, semelhantemente, inviabiliza rejeitar a hipótese de que haja normalidade entre os dados, com um grau de confiabilidade minimamente razoável.

#### Teste de Goldfeld-Quandt

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

t.gq = gqtest(mLstat)

```

Esse teste envolve o ajuste de dois modelos de regressão, separando-se as observações das duas extremidades da distribuição da variável dependente.
Realizado o teste obteve-se um p-valor de aproximadamente `r round(t.gq[[5]][1], 5)`, o que demanda rejeitar a hipótese de que haja homocedasticidade entre os dados, com um grau de confiabilidade de 95%.
Entretanto, como o p-valor obtido é próximo do necessário para a rejeição da hipotese nula, cabe um novo teste para a confirmação do resultado obtido.

#### Teste de Breush-Pagan

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

t.bp = bptest(mLstat, studentize = F)
```

Esse teste é baseado no ajuste de um modelo de regressão em que a variável dependente é definida pelos resíduos do modelo de interesse.
Se grande parte da variabilidade dos resíduos não é explicada pelo modelo, então rejeita-se a hipótese de homocedasticidade.
Realizado o teste obteve-se um p-valor de aproximadamente `r round(t.bp[[4]][1], 5)`, desta foram deve-se rejeitar a hipótese de que haja homocedasticidade entre os dados, com um grau de confiabilidade de 95%.

#### Teste de Park

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false
res2 <- (dados_mLstat_resid$.resid)^2
t.p = summary(lm(res2 ~ dados$lstat))

```

Esse teste é baseado no ajuste de um modelo de regressão em que a variável dependente é definida pelos quadrados dos resíduos do modelo de interesse.
Nesse caso, se $\beta_1$ diferir significativamente de zero, rejeita-se a hipótese de homocedasticidade.
O valor de $\beta_1$ obtido no teste foi de `r round(t.p[[4]][2], 3)` com p-valor de aproximadamente `r round(t.p[[4]][8], 3)`.
Por esse teste não se deve rejeitar a hipótese de homocedasticidade, com confiabilidade de 95%.

#### Teste F para linearidade

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false


m_kmedias <- lm(dados$medv ~ factor(dados$lstat))

t.fl = anova(mLstat, m_kmedias)

```

O teste da falta de ajuste permite testar formalmente a adequação do ajuste do modelo de regressão.
Neste ponto assume-se que os pressupostos de normalidade, variância constante e independência são satisfeitos, como demosntrado pelos testes realizados.
A ideia central para testar a linearidade é decompor SQRes em duas partes: erro puro e falta de ajuste que vão contribuir para a definição da estatística de teste F.
Realizado o teste obteve-se um valore de p-valor igual a 0,289, o que demanda a rejeição da hipótese que há uma relação linear entre as variáveis.

#### Teste para avaliação da independência dos resíduos

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false


t.dw = dwtest(mLstat)
```

Tendo em vista, o resultado obtido no teste anterior esse teste pode esclarecer ainda mais o ajuste do modelo.
O teste para avaliação da independência dos resíduos é utilizado para detectar a presença de autocorrelação provenientes de análise de regressão.
Realizando o teste obteve-se um valor de p-valor aproximadadente igual a `r round(t.dw[[4]][1], 5)`, indicando que se deve rejeitar a hipotese que não existe correlação serial entre os dados, com uma confiança de 95%.

# Análise de Regressão Linear Múltipla Parcial

## Introdução

Nesta parte o conjunto de dados será avaliado de forma múltipla incluindo ao modelo anterior uma variável categórica ao modelo, a fim de selecionar um melhor modelo com o auxílio de uma variável categórica.

## Objetivo

Ajustar um modelo de regressão linear múltiplo utilizando técnicas de retas coincidentes e paralélas a fim de selecionar a melhor explicação para os valores médios dos imóveis de Boston.

## Criação de uma variável categórica

Embora o banco de dados estudado apresente duas variáveis categóricas: a CHAS, que indica a se o imóvel se encontra ou não as margens do rio Charles e RAD, que representa o índice de acessibilidade às rodovias radiais, verificou-se que a correlação destas variáveis com o preço médio dos imóveis é muito pequena, como se pode ver na Figura 4.
Assim pecebeu-se que o tamanho dos imóveis baseado no seu número de cômodos poderia melhor explicar o preço médio destes imóveis.
Desta forma, com base na tabela 1, verificou-se que a mediana do número médio de cômodos dos imóveis poderia ser usada para criar dois grupos: "Imóveis Pequenos" e "Imóveis Maiores".

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

#classificando o tamanho das casas
aux1 <-  dados %>% dplyr::filter(rm<=6.21) %>%  mutate(Imóveis="Menores")
aux2 <-  dados %>% dplyr::filter(rm>6.21) %>%  mutate(Imóveis="Maiores")
dados2 <- rbind(aux1,aux2)
```

## Distribuição do valor médio dos imóveis

Com base na distribuição população de baixa renda e do tamanho dos imóveis segundo a classificação proposta foi avaliado o valor médio dos imóveis conforme as figuras a seguir.

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false
m_nul <- lm(medv ~ 1, data=dados2)
m_coi <- lm(medv ~ lstat, data=dados2)
m_par <- lm(medv ~ lstat + Imóveis, data=dados2)
m_con <- lm(medv ~ lstat * Imóveis, data=dados2)

d_nul <- predict(m_nul,dados2)
d_coi <- predict(m_coi,dados2)
d_par <-  predict(m_par,dados2)
d_con <-  predict(m_con,dados2)
dados2 <- cbind(dados2,d_nul,d_coi,d_par,d_con)

```

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

  ## d1  ----
  d1 <- dados2 |>
    ggplot(aes(y = medv, x = lstat)) +
    geom_point(aes(col=Imóveis))+
    geom_smooth(aes(y=d_nul, col=Imóveis), method=lm, se=TRUE, size = 1)+
    scale_color_brewer(palette = "Paired") +
    labs( title = '',
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    theme_minimal(base_size = 7.5)

  d1 +   
    plot_layout(ncol = 1) + 
    plot_annotation(
      title = "Figura 6: Distribuição entre o valor médio dos imóveis e a \npopulação de baixa renda em função do tamanho do imóvel. \nModelo Nulo") &
    theme(legend.position = c(.8,.8))
```

Utilizando-se o Modelo Nulo, verifica-se pela Figura 6 que há uma diferença expressiva entre os dois sub-conjuntos de dados (classificados segundo o tamanho do imóvel), indicando que há um efeito expressivo da variável categórica no modelo proposto.

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false
#| 
  ## d1  ----
  d1 <- dados2 |>
    ggplot(aes(y = medv, x = lstat)) +
    geom_point(aes(col=Imóveis))+
    geom_smooth(aes(y=d_coi, col=Imóveis), method=lm, se=TRUE, size = 1)+
    scale_color_brewer(palette = "Paired") +
    labs( title = '',
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    theme_minimal(base_size = 7.5)

  d1 +   
    plot_layout(ncol = 1) + 
    plot_annotation(
      title = "Figura 7: Distribuição entre o valor médio dos imóveis e a \npopulação de baixa renda em função do tamanho do imóvel. \nModelo Retas Coincidentes") &
    theme(legend.position = c(.8,.8))


```

No caso da utilização do modelo com retas coincidentes, não se pode pela análise visual definir o efeito dos subconjuntos de dados no modelo proposto, como se vê na Figura 7, uma vez que esse modelo não leva em conta a variável categórica e a reta obtida é baseada nos valores de todo o conjunto.

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

  ## d1  ----
  d1 <- dados2 |>
    ggplot(aes(y = medv, x = lstat)) +
    geom_point(aes(col=Imóveis))+
    geom_smooth(aes(y=d_par, col=Imóveis), method=lm, se=TRUE, size = 1)+
    scale_color_brewer(palette = "Paired") +
    labs( title = '',
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    theme_minimal(base_size = 7.5)

  d1 +   
    plot_layout(ncol = 1) + 
    plot_annotation(
      title = "Figura 8: Distribuição entre o valor médio dos imóveis e a \npopulação de baixa renda em função do tamanho do imóvel. \nModelo Retas Paralelas") &
    theme(legend.position = c(.8,.8))


```

Na Figura 8, há uma clara distinção entre os subconjuntos baseados no tamanho do imóvel, porém este modelo de retas paralelas apresenta retas ainda muito próximas, denotando que o efeito aditivo da classificação é pouco relevante, mesmo que pela análise visual os subconjuntos apresentem características bastante disitintas.

```{r}
#| warning: false
#| eval: true
#| results: false
#| echo: false

# Dispersão ----
{
  ## d1 age ----
  d1 <- dados2 |>
    ggplot(aes(y = medv, x = lstat)) +
    geom_point(aes(col=Imóveis))+
    geom_smooth(aes(y=d_con, col=Imóveis), method=lm, se=TRUE, size = 1)+
    scale_color_brewer(palette = "Paired") +
    labs( title = '',
      y = 'Valor Médio (por $1.000)',
      x = 'Proporção'
    )+
    theme_minimal(base_size = 7.5)

  d1 +   
    plot_layout(ncol = 1) + 
    plot_annotation(
      title = "Figura 9: Distribuição entre o valor médio dos imóveis e a \npopulação de baixa renda em função do tamanho do imóvel. \nModelo Retas Concorrentes") &
    theme(legend.position = c(.8,.8))

}
```

Na Figura 9, observa-se claramente o efeito da interação entre as variáveis.
A sensível diferença entre as inclinações das retas propostas pelo modelo deixam claro os efeitos multiplicativos da interação entre as variáveis. Pode-se verifiar assim que, pela análise gráfica, o modelo de retas concorrentes foi o mais adequado para descrever o comportamento das variáveis selecionadas. Esta observação será verificada sob a ótica da análise dos testes F para os modelos encaixados.


## Análise dos testes F para os modelos encaixados

Após o ajuste dos modelos encaixados existe a necessidade de se avaliar a significância dos mesmos, o teste de hipótese ANOVA para tal situação será realizado. A Tabelas 6 traz os principais resultados da tabela ANOVA para estes modelos, possibilitando assim inferir sobre o modelo mais bem ajustado.

```{r }
#| echo: false
#| warning: false

MyAnova <- function(modelo, nd){
  m1 <- modelo
  np <- dim(anova(m1))[1]
  SQReg <- round(sum(anova(m1)$"Sum Sq"[1:(np-1)]), nd)
  glReg <- sum(anova(m1)$"Df"[1:(np-1)])
  SQRes <- round(anova(m1)$"Sum Sq"[np], nd)
  glRes <- anova(m1)$"Df"[np]
  SQTotal <- round(SQReg + SQRes, nd)
  glTotal <- glReg + glRes
  QMReg <- round(SQReg/glReg, nd)
  QMRes <- round(SQRes/glRes, nd)
  MyF <- round(QMReg/QMRes, nd)
  vpF <- ifelse(pf(MyF, glReg, glRes, lower.tail = F) < 0.0001, "<0.001", 
                round(pf(MyF, glReg, glRes, lower.tail = F), nd))
  ncolunas <- c("Fonte de Variação", "SQ", "gl", "F", "valor p") 
  Tanova <- data.frame(FV = c("Regressão",
                              "Resíduos",
                              "Total"),
                       SQ = c(SQReg, SQRes, SQTotal),
                       gl = c(glReg, glRes, glTotal),
                       QM = c(QMReg, QMRes, " "),
                       Est.F = c(MyF, " ", " "),
                       valor.p = c(vpF, " ", " ")
  )
  Tanova
}

fit_nul <- MyAnova(m_nul, 3) |> as.data.frame()
fit_coi <- MyAnova(m_coi, 3) |> as.data.frame()
fit_par <- MyAnova(m_par, 3) |> as.data.frame()
fit_con <- MyAnova(m_con, 3) |> as.data.frame()

tab <- as.data.frame(c("Nulo","Retas Coincidentes", "Retas Paralélas", "Retas Concorrentes"))
tab <- cbind(tab,c(fit_nul[1,5],fit_coi[1,5],fit_par[1,5],fit_con[1,5]))
tab <- cbind(tab,c(fit_nul[1,6],fit_coi[1,6],fit_par[1,6],fit_con[1,6])) 
colnames(tab) <- c("Modelo", "Est. F", "p-valor")

tab|>
  kbl(
    caption = "Análise de Variância (ANOVA) dos modelos encaixados.",
    align = "lcc", row.names = F, 
    booktabs = T, escape = F
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T
  )|>
    footnote(
    footnote_as_chunk = F
  )|>
  kable_material()

aux <- summary(m_con)

```

Com base nos resultados apresentados na Tabela 6, verifica-se que não é possível definir o melhor modelo apenas pelo teste F aplicado. Nota-se qua a exceção do modelo nulo, todos os demais são, com base neste teste, apropriados para descrever a variável preço médio dos imóveis de Boston com base nas variáveis população de baixa renda e tamanho dos imóveis. Dentre estes, valor da estatística F foi mínimo para o modelo de retas concorrentes que conforme a análise gráfica foi o mais adequado e será selecionado para avaliação diagnóstica do ajuste obtido. Neste modelo o valor do Coeficiente de Determinação de Pearson obtido foi de $R^2=$ `r round(as.numeric(aux$r.square),3)` e do valor do Coeficiente de Determinação de Pearson Ajustado obtido foi de $R^2_{aju}=$ `r round(as.numeric(aux$adj.r.squared),3)`.


## Diagnóstico do ajuste

### Testes de Diagnósticos do Modelo

Para avaliar se o modelo atende aos pressupostos, além da análise gráfica podem ser realizados testes de diagnósticos, que são testes de hipóteses para avaliação dos pressupostos que são:

- Normalidade;

  $H_0:$ Os resíduos possuem normalidade.

  $H_1:$ Os resíduos **não** possuem normalidade.

- Homoscedasticidade (Homogeneidade de Variância);

  $H_0:$ Os resíduos possuem variância constante.

  $H_1:$ Os resíduos **não** possuem variância constante.

- Linearidade;
- Independência.

  $H_0$: Existe correlação serial entre os resíduos.
  
  $H_1$: **Não** existe correlação serial entre os resíduos.

Para tanto serão utilizados os seguintes testes:

- Shapiro-Wilk, para avaliar a Normalidade;
- Breush-Pagan, para avaliar a Homoscedasticidade;
- Durbin-Watson, para avaliar a Independência.


```{r tab4:teste-diagnostico}
#| echo: false
#| warning: false


res2 <- residuals(m_con)

##### Teste de normalidade dos resíduos ----
  #H0: normalidade
  #H1: não normalidade

# SW*
t_sw <- shapiro.test(res2)

##### Teste de homoscedasticidade dos resíduos ----
  #H0: resíduos homoscedásticos - Variância constante
  #H1: resíduos heteroscedásticos - Variância NÃO constante

# BP*
t_bp <- lmtest::bptest(m_con, studentize = F)

# Teste deF para linearidade

# Teste de correlação serial lag 1 (Independência dos erros)
  #H0: correlacionados - existe correlação serial
  #H1: não correlacionados - não existe correlação serial ///ficou confuso no vídeo as hipoteses///

# DW
t_dw <- lmtest::dwtest(m_con)

resultados <- round(rbind(
  t_sw$statistic,
  t_bp$statistic,
  t_dw$statistic),4)

aux <- round(rbind(t_sw$p.value, t_bp$p.value,t_dw$p.value),4)

for (i in 1:3){
  if (aux[i]<0.0001) aux[i] <- "<0,0001"
}

resultados <- cbind(resultados, aux)

rownames(resultados) <- c("Shapiro-Wilk", "Breush-Pagan", "Durbin-Watson")

colnames(resultados) <- c("Estatística de teste", "p-valor")

resultados|>
  kbl(
    caption = "Testes de Diagnósticos dos Resíduos",
    digits = 5,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", row.names = T, booktabs = T
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T)|>
  kable_material()
```

A Tabela 7 traz os testes de diagnósticos realizados para avaliar o modelo de regressão ajustado. Verifica-se que a hipótese de nula da homocedasticidade deve ser rejeitada com um nível de significância de 5%, uma vez que o teste de Breush-Pagam obteve um p-valor menor que 0.05. A normalidade da distribuição também foi rejeitada como indica o p-valor do teste de Shapiro-Wilk. Nota-se ainda que há dependência entre as características confirmado pelo p-valor do teste de Durbin-Watson. Este codependência era esperada uma vez que os tamanhos dos imóveis tem relação com o nível de renda da população. O que pode ser conferdo na matriz de correlação a seguir.


### Correlação entre as variáveis do modelo


A correlação entre as variáveis do modelo pode ser medida pelo coeficiente de correlação entre elas.

```{r}
#| echo: false
#| warning: false
#| tbl-colum: page
#| fig-pos: H
dados3 <- dados2[,c(13,14,15)]
dados3 <- dados3 %>% mutate(Imóveis = ifelse(Imóveis=="Menores",0,1))
b1 <- cor(dados3)

rownames(b1) <- c("População de Baixa Renda","Valor Médio",
                  "Tamanho do Imóvel")
colnames(b1) <- c("População de Baixa Renda","Valor Médio",
                  "Tamanho do Imóvel")

b1|>
  kbl(
    caption = "Matriz de correlação das variáveis do modelo",
    digits = 3,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", row.names = T, booktabs = T
  )|>
  kable_material(c("striped", "hover", "condensed"))|>
  kable_styling(
    bootstrap_options = c("striped", "hover",  "condensed"),
    full_width = F,
    position = 'center', 
    latex_options = c("striped", "HOLD_position", "scale_down","repeat_header")
  ) |>
  column_spec(1, bold = T)|>
  kable_material()
```

Nota-se da Tabela 8 que as variaveis tamanhos dos imóveis tem relação com o nível de renda da população em um nível similar ao com a variável resposta. Esta situação compromete a qualidade do modelo utilizado.



### Análise de Resíduos

```{r}
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3


m_con_resid <- broom::augment(m_con)

####  Gráfico de Resíduos contra Valor Médio
d1 <- m_con_resid|>
 ggplot(aes(x = .fitted, y = rstudent(m_con))) + 
  geom_point(color = "#1F78B4") +
  geom_hline(aes(yintercept = 0), col="tomato")+
  labs(
    x = "Valores Ajustados",
    y = "Resíduos Estudentizados",
    title = "Resíduos Estudentizados vs. \nValores Ajustados")+
  scale_x_continuous(
    labels = scales::number_format(
      big.mark = ".", decimal.mark = ","))+
  scale_y_continuous(
    breaks = seq(from = -3, to = 4, by = 1),
    labels = scales::number_format(
      big.mark = ".", decimal.mark = ","))+
  theme_minimal()+
  theme(
    legend.position = "none",
    plot.title = element_text(size = 11, face = "plain"),
    axis.title = element_text(size = 8, face = "plain"),
    axis.line = element_line(size = 0.5, color = "#222222"))

####  Gráfico de normalidade dos resíduos
 d2 <- m_con_resid %>%
   ggplot(aes(sample = .std.resid)) +
   qqplotr::stat_qq_band(alpha = 0.3) +
   qqplotr::stat_qq_point(color = "#1F78B4") +
   qqplotr::stat_qq_line(linetype = 2, size = 0.2) +
   labs(
     x = "Quantil Teórico",
     y = "Quantil Amostral",
     title = "Gráfico quantil-quantil normal"
   )+
    scale_x_continuous(breaks = seq(-3,3,1))+
   scale_x_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   scale_y_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   theme_minimal()+
   theme(
     legend.position = "none",
     plot.title = element_text(size = 11, face = "plain"),
     axis.title = element_text(size = 8, face = "plain"),
     axis.line = element_line(size = 0.5, color = "#222222"))

#### Gráfico Homogeneidade de Variâncias (Locação-Escala) ----
 d3 <- m_con_resid %>%
   ggplot(aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
   geom_point(color = "#1F78B4") +
   geom_smooth(
     se = T, color = "tomato", method = 'loess', formula = 'y ~ x')+
   labs(
     x = "Valores Ajustados",
     y = expression(sqrt("|Resíduos Padronizados|")),
     title = "Homogeneidade de \n Variâncias \n(Locação-Escala)")+
   scale_x_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   scale_y_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   theme_minimal()+
   theme(
     legend.position = "none",
     plot.title = element_text(size = 11, face = "plain"),
     axis.title = element_text(size = 8, face = "plain"),
     axis.line = element_line(size = 0.5, color = "#222222"))

 d1 + d2 + d3 +
   plot_layout(ncol = 3) +
   plot_annotation(
   title = "Figura 10: Análise de resíduos do modelo ajustado",
   tag_levels = c("A", "2"))&
   theme(
     legend.position = "none",
     plot.tag.position = c(1, 0),
     plot.tag = element_text(size = 12, hjust = 1, vjust = -3)
   )


```



A Figura 10A apresenta um comportamento assimétrico dos resíduos, podendo ser constatado uma pequena variabilidade inicial e um aumento desta à medida que os valores ajustados aumentam, caracterizando uma maior heterocedasticidade. A Figura 10C, que trata da Homogeneidade de Variâncias (Locação-Escala) ressalta que há um problema na variabilidade dos dados, ampliando a interpretação feita na análise da Figura 10A, de que há uma mudança na variabilidade dos dados, caracterizando assim uma heterocedasticidade dos dados. A Figura 10B traz o gráfico para avaliação da normalidade dos dados e mostra que os dados estão bastante afastados da reta de referência, especialmente nas caudas da distribuição onde fogem inclusive da região pertencente ao Intervalo de Confiança - IC, podendo-se assumir que não há normalidade.




### Gráficos de Diagnóstico

A análise dos gráficos de diagnóstico permite avaliar as observações realizadas e conhecer a influência de cada uma delas para o modelo de regressão proposto. Assim, com base no modelo, é possível fazer as seguintes análises:

\newpage

**Figura 11: Valores Ajustados e Resíduos Studentizados**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3
plot_resid_stud <- function (model, print_plot = TRUE) 
{
    check_model(model)
    fct_color <- NULL
    color <- NULL
    obs <- NULL
    dsr <- NULL
    txt <- NULL
    g <- ols_prep_srplot_data(model)
    d <- g$dsr
    d$txt <- ifelse(d$color == "outlier", d$obs, NA)
    f <- d[color == "outlier", c("obs", "dsr")]
    colnames(f) <- c("observação", "resid_stud")
    p <- ggplot(d, aes(x = obs, y = dsr, label = txt)) + geom_bar(width = 0.5, 
        stat = "identity", aes(fill = fct_color)) + scale_fill_manual(values = c("#1F78B4", 
        "tomato")) + xlab("Observação") + ylab("Residuos Studentizados Apagados") + 
        labs(fill = "Observações") + ggtitle("") + 
        ylim(g$cminx, g$cmaxx) + geom_hline(yintercept = c(0, 
        g$nseq, g$pseq)) + geom_hline(yintercept = c(-3, 3), 
        color = "tomato") + geom_text(hjust = -0.2, nudge_x = 0.05, 
        size = 2, na.rm = TRUE) + annotate("text", x = Inf, y = Inf, 
        hjust = 1.2, vjust = 2, family = "serif", fontface = "italic", 
        colour = "darkred", label = paste0("Threshold: abs(", 
            3, ")")) +theme_minimal(base_size = 7.5)
    if (print_plot) {
        suppressWarnings(print(p))
    }
    else {
        return(list(plot = p, outliers = f, threshold = 3))
    }
}


aaa <- plot_resid_stud(m_con)

discrepantes <- NULL
discrepantes[1:506] <- 0

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {  
    discrepantes[i] <- discrepantes[i]+1
  }  
} 

```
A Figura 11 demonstra que os resíduos em sua grande maioria dentro dos limites esperados, com exceção de poucas observações que ultrapassam o limite superior Não parece ser o caso de nenhuma intervenção por conta deste valor.  




**Figura 12: Valores Ajustados e Resíduos Padronizados.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3
plot_resid_stand <- function (model, print_plot = TRUE) 
{
    check_model(model)
    color <- NULL
    obs <- NULL
    sdres <- NULL
    txt <- NULL
    d <- ols_prep_srchart_data(model)
    f <- d[color == "outlier", c("obs", "sdres")]
    colnames(f) <-  c("observação", "resid_stud")
    p <- ggplot(d, aes(x = obs, y = sdres, label = txt, ymin = 0, 
        ymax = sdres)) + geom_linerange(colour = "#1F78B4") + geom_point(shape = 1, 
        colour = "#1F78B4") + geom_hline(yintercept = 0, colour = "gray") + 
        geom_hline(yintercept = c(2, -2), colour = "tomato") + xlab("Observação") + 
        ylab("Residuos Padronizados") + ggtitle("") + 
        geom_text(hjust = -0.2, nudge_x = 0.15, size = 3, family = "serif", 
            fontface = "italic", colour = "darkred", na.rm = TRUE) + 
        annotate("text", x = Inf, y = Inf, hjust = 1.5, vjust = 2, 
            family = "serif", fontface = "italic", colour = "darkred", 
            label = paste0("Threshold: abs(", 2, ")")) +
        theme_minimal(base_size = 7.5)
    if (print_plot) {
        suppressWarnings(print(p))
    }
    else {
        return(list(plot = p, outliers = f, threshold = 2))
    }
}

aaa <- plot_resid_stand(m_con)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {  
    discrepantes[i] <- discrepantes[i]+1
  }  
}
```

Na análise da Figura 12, onde os resíduos foram padronizados, verifica-se queo número de observações além do limite de aceitação aumentou consideravelmente, inclusive o limite inferior tanbém foi ultrapassado. 

\newpage

**Figura 13: Distância de Cook.**

```{r fig17:Cook}
#| echo: false
#| warning: false
#| fig-height: 3
#| fig-width: 7
plot_cooksd_chart <- function (model, print_plot = TRUE) 
{
    check_model(model)
    obs <- NULL
    ckd <- NULL
    txt <- NULL
    cd <- NULL
    k <- ols_prep_cdplot_data(model)
    d <- ols_prep_outlier_obs(k)
    f <- ols_prep_cdplot_outliers(k)
    p <- ggplot(d, aes(x = obs, y = cd, label = txt, ymin = min(cd), 
        ymax = cd)) + geom_linerange(colour = "#1F78B4") + geom_point(shape = 1, 
        colour = "#1F78B4") + geom_hline(yintercept = k$ts, colour = "tomato") + 
        xlab("Observação") + ylab("Distância de Cook") + ggtitle("") + 
        geom_text(vjust = -1, size = 3, family = "serif", fontface = "italic", 
            colour = "darkred", na.rm = TRUE) + annotate("text", 
        x = Inf, y = Inf, hjust = 1.2, vjust = 2, family = "serif", 
        fontface = "italic", colour = "darkred", label = paste("Threshold:", 
            round(k$ts, 3)))+
       theme_minimal(base_size = 7.5)
    if (print_plot) {
        suppressWarnings(print(p))
    }
    else {
        return(list(plot = p, outliers = f, threshold = k$ts))
    }
}



aaa <- plot_cooksd_chart(m_con)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {  
    discrepantes[i] <- discrepantes[i]+1
  }  
}
```

A analise da distância de Cook apresentada na Figura 13 demonstra mais uma vez que muitas observações destoam do conjunto, alguns com distância muito expressiva.  




**Figura 14: Análise dos pontos de Alavanca e Resíduo Studentizado.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

plot_resid_lev <- function (model, print_plot = TRUE) 
{
    check_model(model)
    lev_thrsh <- NULL
    fct_color <- NULL
    leverage <- NULL
    levrstud <- NULL
    txt <- NULL
    obs <- NULL
    color <- NULL
    resp <- names(model.frame(model))[1]
    title <- ""
    g <- ols_prep_rstudlev_data(model)
    ann_paste <- round(g$lev_thrsh, 3)
    ann_label <- paste("Threshold:", ann_paste)
    d <- g$levrstud
    d$txt <- ifelse(d$color == "normal", NA, d$obs)
    f <- d[d$color == "outlier", c("obs", "leverage", "rstudent")]
    colnames(f) <- c("observação", "leverage", "resid_stud")
    p <- ggplot(d, aes(leverage, rstudent, label = txt)) + geom_point(shape = 1, 
        aes(colour = fct_color)) + scale_color_manual(values = c("#1F78B4", 
        "tomato", "green", "violet")) + xlim(g$minx, g$maxx) + ylim(g$miny, 
        g$maxy) + labs(colour = "Observação") + xlab("Leverage") + 
        ylab("RStudent") + ggtitle(title) + geom_hline(yintercept = c(2, 
        -2), colour = "maroon") + geom_vline(xintercept = g$lev_thrsh, 
        colour = "maroon") + geom_text(vjust = -1, size = 3, 
        family = "serif", fontface = "italic", colour = "darkred") + 
        annotate("text", x = Inf, y = Inf, hjust = 1.2, vjust = 2, 
            family = "serif", fontface = "italic", colour = "darkred", 
            label = ann_label)+
        theme_minimal(base_size = 7.5)
    if (print_plot) {
        suppressWarnings(print(p))
    }
    else {
        return(list(plot = p, leverage = f, threshold = g$lev_thrsh))
    }
}

aaa <- plot_resid_lev(m_con)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {  
    discrepantes[i] <- discrepantes[i]+1
  }  
}
```

Pela Figura 14, observamos que diversas observações que podem ser consideradas como *Outliers* e como pontos de alavanca e algumas com as duas características sendo possívelmente as observações com maior influência negativa ao modelo. 

\newpage

**Figura 15: DFBetas para as variáveis do modelo.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 4

plot_dfbetas <- function (model, print_plot = TRUE) 
{
    check_model(model)
    obs <- NULL
    txt <- NULL
    dfb <- dfbetas(model)
    n <- nrow(dfb)
    np <- ncol(dfb)
    threshold <- 2/sqrt(n)
    myplots <- list()
    outliers <- list()
    for (i in seq_len(np)) {
        dbetas <- dfb[, i]
        df_data <- data.frame(obs = seq_len(n), dbetas = dbetas)
        d <- ols_prep_dfbeta_data(df_data, threshold)
        f <- ols_prep_dfbeta_outliers(d)
        p <- eval(substitute(ggplot(d, aes(x = obs, y = dbetas, 
            label = txt, ymin = 0, ymax = dbetas)) + geom_linerange(colour = "#1F78B4") + 
            geom_hline(yintercept = c(0, threshold, -threshold), 
                colour = "tomato") + geom_point(colour = "#1F78B4", 
            shape = 1) + xlab("Observação") + ylab("DFBETAS") + 
            ggtitle(paste("Diagnóstico de Influência", colnames(dfb)[i])) + 
            geom_text(hjust = -0.2, nudge_x = 0.15, size = 2, 
                family = "serif", fontface = "italic", colour = "darkred", 
                na.rm = TRUE) + annotate("text", x = Inf, y = Inf, 
            hjust = 1.5, vjust = 2, family = "serif", fontface = "italic", 
            colour = "darkred", label = paste("Threshold:", round(threshold, 
                2))), list(i = i)))+
            theme_minimal(base_size = 7.5)
        myplots[[i]] <- p
        outliers[[i]] <- f
    }
    if (print_plot) {
        marrangeGrob(myplots, nrow = 2, ncol = 2)
    }
    else {
        return(list(plots = myplots, outliers = outliers))
    }
}


plot_dfbetas(m_con)


showcases<-data.frame(dfbetas(m_con))
showcases$ID<-rownames(showcases)

for (i in 1:505) {
  if (abs(showcases$lstat[i])>.09) {  
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$ImóveisMenores[i])>.09) {  
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$lstat.ImóveisMenores[i])>.09) {  
    discrepantes[i] <- discrepantes[i]+1
  }
    if (abs(showcases$X.Intercept.[i])>.09) {  
    discrepantes[i] <- discrepantes[i]+1
  }
}
```

A Figura 15 apresenta os DFBetas para cada uma das variáveis utilizadas no modelo. Nota-se que as observações já identificadas como anômalas pelos gráficos anteriores se repetem com maior frequência na Figura 15.



**Figura 16: DfFit para as variáveis do modelo.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

model_n_coeffs <- function(model) {
  length(model$coefficients)
}
model_rows <- function(model) {
  nrow(model.frame(model))
}


plot_dffits <- function (model, print_plot = TRUE) 
{
    check_model(model)
    dbetas <- NULL
    obs <- NULL
    txt <- NULL
    dffitsm <- unlist(dffits(model))
    k <- model_n_coeffs(model)
    n <- model_rows(model)
    dffits_t <- sqrt(k/n) * 2
    title <- names(model.frame(model))[1]
    dfits_data <- data.frame(obs = seq_len(n), dbetas = dffitsm)
    d <- ols_prep_dfbeta_data(dfits_data, dffits_t)
    f <- ols_prep_dfbeta_outliers(d)
    p <- ggplot(d, aes(x = obs, y = dbetas, label = txt, ymin = 0, 
        ymax = dffitsm)) + geom_linerange(colour = "#1F78B4") + 
        geom_hline(yintercept = c(0, dffits_t, -dffits_t), colour = "tomato") + 
        geom_point(colour = "#1F78B4", shape = 1) + xlab("Observação") + 
        ylab("DFFITS") + ggtitle("") + 
        geom_text(hjust = -0.2, nudge_x = 0.15, size = 3, 
        family = "serif", fontface = "italic", colour = "darkred", 
        na.rm = TRUE) + annotate("text", x = Inf, y = Inf, hjust = 1.5, 
        vjust = 2, family = "serif", fontface = "italic", colour = "darkred", 
        label = paste("Threshold:", round(dffits_t, 2)))+
        theme_minimal(base_size = 7.5)
    if (print_plot) {
        suppressWarnings(print(p))
    }
    else {
        colnames(f) <- c("observação", "dffits")
        return(list(plot = p, outliers = f, threshold = round(dffits_t, 
            2)))
    }
}



aaa <- plot_dffits(m_con)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {  
    discrepantes[i] <- discrepantes[i]+1
  }  
}

```

A Figura 16 acompanha os gráficos anteriores apresentando mais uma vez muitas observações discrepante.       

Este comportamento com muitas observações discrepantes deve-se ao pressupostos da regressão linear múltipla terem sido violados como mostraram a Figura 10 e a Tabela 8.


### Eliminação de observações anômalas

A fim de melhorar o ajuste do modelo, identificando ss observações que apresentaram comportamento anômalo nos diagnósticos dos valores ajustados e resíduos studentizados, valores ajustados e resíduos padronizados, distância de CooK, pontos de alavanca e *outliers*, análise de DfFit e todas as análises de BFBetas, chegamos as frequências de observações anômalas apresentadas na Figura 17.
```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

principais <- as.data.frame(cbind(1:506, discrepantes))
colnames(principais) <- c("Observação", "Ocorrências")

### Ajuste do Modelo + Gráfico ----
principais|>
  ggplot(aes(x = Observação, y = Ocorrências)) +
  geom_point(
    color = "#1F78B4"
    )+
  labs(
    title = "Figura 17: Número de ocorrências para cada observação",
    y = 'Ocorrências',
    x = 'Observação'
  )+
  theme(legend.position = "none",
          axis.line = element_line(size = 0.5, color = "#222222"))


```

Considerando apenas as observações com 5 ou mais ocorrências temos a Tabela a seguir.

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

principais <- principais %>% filter(discrepantes>4)

principais |>
  kableExtra::kbl(
    caption = "Observações com maior número de ocorrências.",
    align = c("l", "c"), 
    row.names = F, booktabs = T, escape = F
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position")
  )|>
   kable_material()

```


```{r }
#| echo: false
#| warning: false
dados4 <- dados3[-principais$Observação,]
m_con_aju <- lm(medv ~ lstat * Imóveis, data =dados4)
aux <- summary(m_con_aju)
r2 <- round(as.numeric(aux$r.squared),3)
r2a <- round(as.numeric(aux$adj.r.squared),3)

```

Observou-se que alguns dos pontos elencados na Tabela 9, correspondem a *outliers* também verificados na análise descritiva dos dados (Figura 3). A não eliminação dos pontos atípicos naquele momento era desaconselhável, entretanto após as análises realisadas há maior segurança no procedimento. Essas observaçãoes correspondem a 6,9% do total de observações do conjunto, desta forma entende-se que a eliminação delas não provocará uma perda significativa de informação para o modelo.  
Desta forma, obteve-se o seguinte modelo resultante:  


$Y_{i}=$ `r round(aux$coefficients[1,1],3)` `r round(aux$coefficients[2,1],3)` $X_{1i}+$ `r round(aux$coefficients[3,1],3)` $X_{2i}$ `r round(aux$coefficients[4,1],3)` $X_{1,2i}$ 

Onde:    
$Y_{i}$ - Valor médio dos imóveis de Boston;  
$X_{1i}$ - População de Baixa Renda;  
$X_{2i}$ - Tamanho do Imóvel;  
$X_{1,2i}$ - Interação entre as variáveis explicativas ($X_{1i}*X_{2i}$);  




Neste novo modelo o coeficiente de determinação calculado foi de $R^2=$ `r r2`, o que denota que `r r2*100`% da variância dos dados é explicada pelo modelo. O valor deste novo coeficiente permite concluir que a eliminação das observações com maior impacto e das variáveis pouco relevantes ao modelo foi benéfica. Pode-se calcular o coeficiente de determinação ajustado igual a $R^2_{aju}=$ `r r2a`. Estes valores indicam que a eliminação das observações anômalas contribuiu signicativamente para a melhora do modelo proposto.

# Análise de Regressão Linear Múltipla

## Introdução

Nesta parte o conjunto de dados será avaliado de forma múltipla incluindo ao modelo todas as variáveis disponíveis, a fim de selecionar um melhor subconjunto de variáveis para modelo de regressão.

## Objetivo

Ajustar um modelo de regressão linear múltiplo utilizando técnicas trabalhadas na dsciplina de Analise de Regressão Linear do curso de Estatística da UFBA, a fim de selecionar a melhor explicação para os valores médios dos imóveis de Boston.


## Avaliação do modelo completo

Com base no banco de dados completo é posasível propor um modelo geral que apresenta as propriedades descritas na Tabela a seguir. O ajuste dos modelo pode ter sua significância avaliada pelo teste de hipótese ANOVA. 



```{r}
#| echo: false
#| warning: false

dados4 <- dados2[,-c(16,17,18,19)]
dados4$chas <- as.numeric(dados4$chas)-1
dados4$rad <- as.numeric(dados4$rad)
dados4[dados4$rad==9,9] <- 24
dados4$Imóveis <- abs(as.numeric(as.factor(dados4$Imóveis))-2)
m.total <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + 
                tax + ptratio + b + lstat, data=dados4)

```



```{r }
#| echo: false
#| warning: false

MyAnova <- function(modelo, nd){
  m1 <- modelo
  np <- dim(anova(m1))[1]
  SQReg <- round(sum(anova(m1)$"Sum Sq"[1:(np-1)]), nd)
  glReg <- sum(anova(m1)$"Df"[1:(np-1)])
  SQRes <- round(anova(m1)$"Sum Sq"[np], nd)
  glRes <- anova(m1)$"Df"[np]
  SQTotal <- round(SQReg + SQRes, nd)
  glTotal <- glReg + glRes
  QMReg <- round(SQReg/glReg, nd)
  QMRes <- round(SQRes/glRes, nd)
  MyF <- round(QMReg/QMRes, nd)
  vpF <- ifelse(pf(MyF, glReg, glRes, lower.tail = F) < 0.0001, "<0.001", 
                round(pf(MyF, glReg, glRes, lower.tail = F), nd))
  ncolunas <- c("Fonte de Variação", "SQ", "gl", "F", "valor p") 
  Tanova <- data.frame(FV = c("Regressão",
                              "Resíduos",
                              "Total"),
                       SQ = c(SQReg, SQRes, SQTotal),
                       gl = c(glReg, glRes, glTotal),
                       QM = c(QMReg, QMRes, " "),
                       Est.F = c(MyF, " ", " "),
                       valor.p = c(vpF, " ", " ")
  )
  Tanova
}

fit.total <- MyAnova(m.total, 3) |> as.data.frame()



fit.total|>
  kbl(
    caption = "Análise de Variância (ANOVA) do modelo completo.",
    align = "lccccc", row.names = F, 
    booktabs = T, escape = F
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T
  )|>
    footnote(
    footnote_as_chunk = F
  )|>
  kable_material()

aux <- summary(m.total)

```

Com base nos resultados apresentados na Tabela 10, verifica-se que o modelo completo é significativo com base no teste F aplicado. Neste modelo o valor do Coeficiente de Determinação de Pearson obtido foi de $R^2=$ `r round(as.numeric(aux$r.square),3)` e do valor do Coeficiente de Determinação de Pearson Ajustado obtido foi de $R^2_{aju}=$ `r round(as.numeric(aux$adj.r.squared),3)`. 


## Variáveis Estatisticamente Significantes


Considerando um teste de hipótese para os parâmetros individuais do modelo podemos avaliar se: 

$$H_0 : \beta_j = 0$$ 
$$H_1 : \beta_j \ne 0$$

Utilizando a estatística teste dada por:

$$t= \dfrac{\hat \beta_j - \beta_j}{ep(\hat \beta_j)}$$
Com base no valor tabelado de $t_{(2,5\%,491)}=$ `r round(qt(0.025, 491), 3)` e realizados os calculos verificou-se os seguintes valores da estatística t:  

```{r }
#| echo: false
#| warning: false

n <- length(dados4$lstat)
Y <- as.matrix(dados4$medv)
X <- as.matrix(dados4[,-14])
X <- cbind(1,X)
betas <- (solve(t(X) %*% X)) %*% t(X) %*% Y
H = X %*% (solve(t(X) %*% X)) %*% t(X)
J <- matrix(1,n,n)
SQRes <- t(Y) %*% Y - t(betas) %*% t(X) %*% Y
SQReg <- t(betas) %*% t(X) %*% Y - (1/n)*t(Y) %*% J %*% Y
SQTot <- SQReg + SQRes

R2 <- round(SQReg/SQTot,3)
R2_aju <- round(1 - (SQRes/(n - 15))/(SQTot/(n -1)),3)

sigma2_hat <- SQRes/(n-15)
ep_betas <- sqrt(sigma2_hat*diag(solve(t(X) %*% X)))
t <- t(betas)/ep_betas


colnames(t) <- c("Beta 0","Índice Criminalidade","Prop. Terreno Zoneado",
                 "Área Industrial","Margem", "Índice Oxido Nítrico", "N° Cômodos",
                 "Idade do Imóvel", "Dist. Empregos", "Acessibilidade Rodovias",
                 "Imposto Propriedade", "Prop. Prof.-Aluno", "Prop. Negros/bairro",
                 "Pop. Classe Baixa", "Tamanho dos Imóveis")
  
t <-  as.data.frame(t(t))

pvalor <- NULL
for (i in 1:15){
  pvalor[i] <- round(pt(-abs(t[i,1]),491),4)
  if (abs(as.numeric(pvalor[i]))<0.0001){
    pvalor[i] <- "<0,0001"
  }  
}

t <- cbind(t,pvalor)

t|>
 kbl(
    caption = "Análise de Significância",
    digits = 3,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "cc", 
    col.names = c("Estatística t", "p-Valor"),
    row.names = T, 
    booktabs = T
  )|>
  column_spec(1, bold = T)|>
  kable_styling(
    full_width = F,
    position = 'center', 
    latex_options = c("striped", "HOLD_position")
  )|>
  kable_material()
```



```{r}
#| echo: false
#| warning: false

cp.mellows <- model.matrix(m.total)[,-1]
estat <- leaps::leaps(cp.mellows, dados4$medv)
result.cp <- cbind(estat$Cp,estat$which)
aux <- result.cp[which.min(result.cp[,1]),]
aux <- aux[1]
aux[2] <- result.cp[121,1]
```
Nota-se, da Tabela 11, que as seguintes variáveis não se mostraram estatisticamente significantes ao nível de significancia de 5%: Idade do Imóvel e se o imóvel se encontra em Área Industrial.  
Usando a técnica de $C_p$ de Mellows, para a qual todos os submodelos das covariáveis são testados, de mameira a identificar o modelo com menor valor de $C_p$. Observou-se que o $C_p$ do modelo completo foi de `r round(aux[2],3)`, porém um valor de $C_p=$ `r round(aux[1],3)` foi obtido em um submodelo para o qual as variáveis Idade do Imóvel e Área Industrial foram removidas. Confiremando assim a viabilidade de eliminar essas variáveis.

Um novo modelo sem essas variáveis pode ser representado por:


```{r }
#| echo: false
#| warning: false

m.final <- lm(medv ~ crim + zn + chas + nox + rm + dis + rad + 
                tax + ptratio + b + lstat, data=dados4)
fit.final <- MyAnova(m.final, 3) |> as.data.frame()



fit.final|>
  kbl(
    caption = "Análise de Variância (ANOVA) do modelo completo.",
    align = "lccccc", row.names = F, 
    booktabs = T, escape = F
  )|>
  kable_styling(
    full_width = F, position = 'center', 
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T
  )|>
    footnote(
    footnote_as_chunk = F
  )|>
  kable_material()

aux <- summary(m.final)

```
                
Com base nos resultados apresentados na Tabela 12, verifica-se que o modelo ajustado é significativo com base no teste F aplicado. Neste modelo o valor do Coeficiente de Determinação de Pearson obtido foi de $R^2=$ `r round(as.numeric(aux$r.square),3)` e do valor do Coeficiente de Determinação de Pearson Ajustado obtido foi de $R^2_{aju}=$ `r round(as.numeric(aux$adj.r.squared),3)`.                


## Diagnóstico do ajuste

### Testes de Diagnósticos do Modelo

Para avaliar se o modelo atende aos pressupostos, além da análise gráfica podem ser realizados testes de diagnósticos, que são testes de hipóteses para avaliação dos pressupostos que são:

- Normalidade;

  $H_0:$ Os resíduos possuem normalidade.

  $H_1:$ Os resíduos **não** possuem normalidade.

- Homoscedasticidade (Homogeneidade de Variância);

  $H_0:$ Os resíduos possuem variância constante.

  $H_1:$ Os resíduos **não** possuem variância constante.

- Linearidade;
- Independência.

  $H_0$: Existe correlação serial entre os resíduos.

  $H_1$: **Não** existe correlação serial entre os resíduos.

Para tanto serão utilizados os seguintes testes:

- Shapiro-Wilk, para avaliar a Normalidade;
- Breush-Pagan, para avaliar a Homoscedasticidade;
- Durbin-Watson, para avaliar a Independência.


```{r}
#| echo: false
#| warning: false


res2 <- residuals(m.final)

##### Teste de normalidade dos resíduos ----
  #H0: normalidade
  #H1: não normalidade

# SW*
t_sw <- shapiro.test(res2)

##### Teste de homoscedasticidade dos resíduos ----
  #H0: resíduos homoscedásticos - Variância constante
  #H1: resíduos heteroscedásticos - Variância NÃO constante

# BP*
t_bp <- lmtest::bptest(m.final, studentize = F)

# Teste deF para linearidade

# Teste de correlação serial lag 1 (Independência dos erros)
  #H0: correlacionados - existe correlação serial
  #H1: não correlacionados - não existe correlação serial ///ficou confuso no vídeo as hipoteses///

# DW
t_dw <- lmtest::dwtest(m.final)

resultados <- round(rbind(
  t_sw$statistic,
  t_bp$statistic,
  t_dw$statistic),4)

aux <- round(rbind(t_sw$p.value, t_bp$p.value,t_dw$p.value),4)

for (i in 1:3){
  if (aux[i]<0.0001) aux[i] <- "<0,0001"
}

resultados <- cbind(resultados, aux)

rownames(resultados) <- c("Shapiro-Wilk", "Breush-Pagan", "Durbin-Watson")

colnames(resultados) <- c("Estatística de teste", "p-valor")

resultados|>
  kbl(
    caption = "Testes de Diagnósticos dos Resíduos",
    digits = 5,
    format.args=list(big.mark=".", decimal.mark=","),
    align = "c", row.names = T, booktabs = T
  )|>
  kable_styling(
    full_width = F, position = 'center',
    latex_options = c("striped", "HOLD_position", "repeat_header")
  )|>
  column_spec(1, bold = T)|>
  kable_material()
```

A Tabela 13 traz os testes de diagnósticos realizados para avaliar o modelo de regressão ajustado. Verifica-se que a hipótese nula da homocedasticidade deve ser rejeitada com um nível de significância de 5%, uma vez que o teste de Breush-Pagam obteve um p-valor menor que 0,05. A normalidade da distribuição também foi rejeitada como indica o p-valor do teste de Shapiro-Wilk. Nota-se ainda que há dependência entre as características confirmado pelo p-valor do teste de Durbin-Watson. Como estes pressupostos básicos não se verificaram, um Modelo de Regressão Linear Múltipla não deveria ser utilizado para a explicação do valor dos imóveis em Boston. Entretanto, como exercício de análise de um modelo RLM as análises prosseguirão.

### Análise de Resíduos

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3


m_fim_resid <- broom::augment(m.final)

####  Gráfico de Resíduos contra Valor Médio
d1 <- m_fim_resid|>
 ggplot(aes(x = .fitted, y = rstudent(m_con))) +
  geom_point(color = "#1F78B4") +
  geom_hline(aes(yintercept = 0), col="tomato")+
  labs(
    x = "Valores Ajustados",
    y = "Resíduos Estudentizados",
    title = "Resíduos Estudentizados vs. \nValores Ajustados")+
  scale_x_continuous(
    labels = scales::number_format(
      big.mark = ".", decimal.mark = ","))+
  scale_y_continuous(
    breaks = seq(from = -3, to = 4, by = 1),
    labels = scales::number_format(
      big.mark = ".", decimal.mark = ","))+
  theme_minimal()+
  theme(
    legend.position = "none",
    plot.title = element_text(size = 11, face = "plain"),
    axis.title = element_text(size = 8, face = "plain"),
    axis.line = element_line(size = 0.5, color = "#222222"))

####  Gráfico de normalidade dos resíduos
 d2 <- m_fim_resid %>%
   ggplot(aes(sample = .std.resid)) +
   qqplotr::stat_qq_band(alpha = 0.3) +
   qqplotr::stat_qq_point(color = "#1F78B4") +
   qqplotr::stat_qq_line(linetype = 2, size = 0.2) +
   labs(
     x = "Quantil Teórico",
     y = "Quantil Amostral",
     title = "Gráfico quantil-quantil normal"
   )+
    scale_x_continuous(breaks = seq(-3,3,1))+
   scale_x_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   scale_y_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   theme_minimal()+
   theme(
     legend.position = "none",
     plot.title = element_text(size = 11, face = "plain"),
     axis.title = element_text(size = 8, face = "plain"),
     axis.line = element_line(size = 0.5, color = "#222222"))

#### Gráfico Homogeneidade de Variâncias (Locação-Escala) ----
 d3 <- m_fim_resid %>%
   ggplot(aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
   geom_point(color = "#1F78B4") +
   geom_smooth(
     se = T, color = "tomato", method = 'loess', formula = 'y ~ x')+
   labs(
     x = "Valores Ajustados",
     y = expression(sqrt("|Resíduos Padronizados|")),
     title = "Homogeneidade de \n Variâncias \n(Locação-Escala)")+
   scale_x_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   scale_y_continuous(
     labels = scales::number_format(
       big.mark = ".", decimal.mark = ","))+
   theme_minimal()+
   theme(
     legend.position = "none",
     plot.title = element_text(size = 11, face = "plain"),
     axis.title = element_text(size = 8, face = "plain"),
     axis.line = element_line(size = 0.5, color = "#222222"))

 d1 + d2 + d3 +
   plot_layout(ncol = 3) +
   plot_annotation(
   title = "Figura 18: Análise de resíduos do modelo ajustado",
   tag_levels = c("A", "2"))&
   theme(
     legend.position = "none",
     plot.tag.position = c(1, 0),
     plot.tag = element_text(size = 12, hjust = 1, vjust = -3)
   )


```



A Figura 18A apresenta um comportamento aproximandamente simétrico dos resíduos, podendo ser constatado uma pequena variabilidade à medida que os valores ajustados aumentam, caracterizando uma possível homocedasticidade. Entretanto, a Figura 18C, que trata da Homogeneidade de Variâncias (Locação-Escala) ressalta que há um problema na variabilidade dos dados, ampliando a interpretação feita na análise da Figura 10A, de que há uma mudança na variabilidade dos dados, caracterizando assim alguma heterocedasticidade dos dados, que confirma o teste de Breush-Pagan realizado. A Figura 18B traz o gráfico para avaliação da normalidade dos dados e mostra que os dados estão bastante afastados da reta de referência, especialmente nas caudas da distribuição onde fogem inclusive da região pertencente ao Intervalo de Confiança - IC, podendo-se assumir que não há normalidade, como já observado pelo resultado do teste de Shapiro-Wilk.  

### Gráficos de Diagnóstico

A análise dos gráficos de diagnóstico permite avaliar as observações realizadas e conhecer a influência de cada uma delas para o modelo de regressão proposto. Assim, com base no modelo, é possível fazer as seguintes análises:

**Figura 19: Valores Ajustados e Resíduos Studentizados**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

aaa <- plot_resid_stud(m.final)

discrepantes <- NULL
discrepantes[1:506] <- 0

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {
    discrepantes[i] <- discrepantes[i]+1
  }
}

```
A Figura 19 demonstra que os resíduos em sua grande maioria dentro dos limites esperados, com exceção de poucas observações que ultrapassam o limite superior Não parece ser o caso de nenhuma intervenção por conta deste valor.

\newpage


**Figura 20: Valores Ajustados e Resíduos Padronizados.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

aaa <- plot_resid_stand(m.final)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {
    discrepantes[i] <- discrepantes[i]+1
  }
}
```

Na análise da Figura 20, onde os resíduos foram padronizados, verifica-se que o número de observações além do limite de aceitação aumentou consideravelmente, inclusive o limite inferior foi ultrapassado de maneira significativa.



**Figura 21: Distância de Cook.**

```{r }
#| echo: false
#| warning: false
#| fig-height: 3
#| fig-width: 7

aaa <- plot_cooksd_chart(m.final)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {
    discrepantes[i] <- discrepantes[i]+1
  }
}
```

A analise da distância de Cook apresentada na Figura 21 demonstra mais uma vez que muitas observações destoam do conjunto, alguns com distância muito expressiva.

\newpage

**Figura 22: Análise dos pontos de Alavanca e Resíduo Studentizado.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

aaa <- plot_resid_lev(m.final)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {
    discrepantes[i] <- discrepantes[i]+1
  }
}
```

Pela Figura 22, observamos que diversas observações que podem ser consideradas como *Outliers* e como pontos de alavanca e algumas com as duas características sendo possívelmente as observações com maior influência negativa ao modelo.



**Figura 23: DFBetas para as variáveis do modelo.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 4

plot_dfbetas(m.final)


showcases<-data.frame(dfbetas(m.final))
showcases$ID<-rownames(showcases)

for (i in 1:505) {
  if (abs(showcases$X.Intercept.[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$crim[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$zn[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$chas[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$nox[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$rm[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$dis[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$rad[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$tax[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$ptratio[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }
  if (abs(showcases$b[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }  
  if (abs(showcases$lstat[i])>.09) {
    discrepantes[i] <- discrepantes[i]+1
  }  
}
```

A Figura 23 apresenta os DFBetas para cada uma das variáveis utilizadas no modelo. Nota-se que as observações já identificadas como anômalas pelos gráficos anteriores se repetem com maior frequência na Figura.

\newpage

**Figura 24: DfFit para as variáveis do modelo.**

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

aaa <- plot_dffits(m.final)

for (i in 1:506) {
  if (!is.na(aaa[["data"]][["txt"]][i])) {
    discrepantes[i] <- discrepantes[i]+1
  }
}

```

A Figura 24 acompanha os gráficos anteriores apresentando mais uma vez muitas observações discrepante.

Este comportamento com muitas observações discrepantes deve-se ao pressupostos da regressão linear múltipla terem sido violados como mostraram a Figura 18 e a Tabela 13.


### Eliminação de observações anômalas

A fim de melhorar o ajuste do modelo, identificando ss observações que apresentaram comportamento anômalo nos diagnósticos dos valores ajustados e resíduos studentizados, valores ajustados e resíduos padronizados, distância de CooK, pontos de alavanca e *outliers*, análise de DfFit e todas as análises de BFBetas, chegamos as frequências de observações anômalas apresentadas na Figura 25.
```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

principais <- as.data.frame(cbind(1:506, discrepantes))
colnames(principais) <- c("Observação", "Ocorrências")

### Ajuste do Modelo + Gráfico ----
principais|>
  ggplot(aes(x = Observação, y = Ocorrências)) +
  geom_point(
    color = "#1F78B4"
    )+
  labs(
    title = "Figura 25: Número de ocorrências para cada observação",
    y = 'Ocorrências',
    x = 'Observação'
  )+
  theme(legend.position = "none",
          axis.line = element_line(size = 0.5, color = "#222222"))


```

Considerando apenas as observações com 5 ou mais ocorrências temos a Tabela a seguir.

```{r }
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3

principais <- principais %>% filter(discrepantes>4)

principais |>
  kableExtra::kbl(
    caption = "Observações com maior número de ocorrências.",
    align = c("l", "c"),
    row.names = F, booktabs = T, escape = F
  )|>
  kable_styling(
    full_width = F, position = 'center',
    latex_options = c("striped", "HOLD_position")
  )|>
   kable_material()

```


```{r }
#| echo: false
#| warning: false


dados5 <- dados4[-principais$Observação,]
m.final.aju <- lm(medv ~ crim + zn + chas + nox + rm + dis + rad + 
                tax + ptratio + b + lstat, data=dados4)
aux <- summary(m.final.aju)
r2 <- round(as.numeric(aux$r.squared),3)
r2a <- round(as.numeric(aux$adj.r.squared),3)

```

Observou-se que alguns dos pontos elencados na Tabela 14, correspondem a *outliers* também verificados na análise descritiva dos dados (Figura 3). A não eliminação dos pontos atípicos naquele momento era desaconselhável, entretanto após as análises realisadas há maior segurança no procedimento. Essas observaçãoes correspondem a 6,7% do total de observações do conjunto, desta forma entende-se que a eliminação delas não provocará uma perda significativa de informação para o modelo.




Neste novo modelo o coeficiente de determinação calculado foi de $R^2=$ `r r2`, o que denota que `r r2*100`% da variância dos dados é explicada pelo modelo. O valor deste novo coeficiente permite concluir que a eliminação das observações com maior impacto e das variáveis pouco relevantes ao modelo foi benéfica. Pode-se calcular o coeficiente de determinação ajustado igual a $R^2_{aju}=$ `r r2a`. Estes valores indicam que a eliminação das observações anômalas contribuiu signicativamente para a melhora do modelo proposto.

\newpage

# Conclusão

### Análise de Regressão Linear Simples

Da análise descritiva das variáveis deste banco de dados não se observa, situações impeditivas da proposta de modelamento por Regressão Linear Simples dos dados como forma de predizer o Valor Médio dos Imóveis de Boston.
Mesmo a análise de valores atípicos contribui com essa possibilidade uma vez que os valores candidatos a valores atípicos na verdade compõem o rol de dados relevantes e que há enorme variedade em tipos, propósitos e status dos imóveis avaliados.
Esses dados por sua vez, representam um maior desafio ao modelamento a que esse trabalho se propõe.\
No teste da hipótese de correlação, todas as variáveis apresentaram significativa relação linear com o valor médio do imóvel, mesmo em casos que o coeficiente de determinação ($R^2$) se apresentou muito baixo.\
A implementação de técnicas de Regressão Linear Simples - RLS, para a variável explicativa que aparenta melhor possibilidade de explicação do Preço Médio dos Imóveis - Pop.
Classe Baixa, não se mostrou muito eficiente como observado pela análise gráfica dos resíduos.
Para melhor compreensão desta análise foram feitos testes de normalidade, homoscedasticidade e de independência dos resíduos, de onde se concluiu que se deve rejeitar as hipóteses de normalidade, homoscedasticidade e de independência serial dos dados, confirmando assim o que a análise gráfica demonstrou.

### Análise de Regressão Linear Múltipla

A criação da variável Tamanho do Imóvel, dividindo os imóveis em dois grupos equivalentes, quanto ao número médio de cômodos, foi bastante útil para os propósitos desta segunda parte do relatório.    

A análise gráfica para a determinação do modelo retas concorrentes foi mais útil que a realização do teste F, uma vez que os resultados obtidos foram os mesmos para todos os modelos análisados, a exceção do modelo nulo.  

O ajuste do modelo aos pressupostos da Regressão Linear Múltipla não foi satisfatório e o proseguimento dos calculos visou apenas observar o comportamento do modelo. Entretanto, a eliminação de observações anômalas fez com que a qualidade do modelo proposto fosse significativamente melhorada.

### Análise de Regressão Linear Múltipla



A avaliação do coeficiente de determinação ao longo de todo o projeto revelou que no modelo baseado em apenas uma das variáveis, a Classe Baixa, atingiu um $R^2=0,544$. O modelo que combinou uma variável quantitativa contínua e uma varável categórica obteve um valor de $R^2=0,713$. Por fim o modelo completo obteve um $R^2=0,741$. O ajuste eliminando duas variáveis pouco significativas, manteve esse mesmo valor apresentando mínima variação no valor do $R^2_{aju}$. Mesmo com a eliminação de pontos atípicos não se observou melhora no valor de $R^2$ que se manteve em 0,741. Desta forma, pelo princípio da parcimônia é correto afirmar que o modelo com apenas duas variáveis explicativas é o mais adequado para descrever o comportamento do preço médio dos imóveis de Boston.






# Referências

-   Harrison, David & Rubinfeld, Daniel. (1978). Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Management. 5. 81-102. 10.1016/0095-0696(78)90006-2.

-   Belsley, David A. & Kuh, Edwin. & Welsch, Roy E. (1980). Regression diagnostics: identifying influential data and sources of collinearity. New York: Wiley.
